<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-16T17:19:52+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Something I found</title><subtitle>There is no description for this lxj616's blog, lazy dude, nah</subtitle><entry><title type="html">Scaling up longer video generation model training without more gpus</title><link href="http://localhost:4000/jekyll/update/2023/07/09/scaling-up-longer-video-generation-without-more-gpus.html" rel="alternate" type="text/html" title="Scaling up longer video generation model training without more gpus" /><published>2023-07-09T21:46:14+08:00</published><updated>2023-07-09T21:46:14+08:00</updated><id>http://localhost:4000/jekyll/update/2023/07/09/scaling-up-longer-video-generation-without-more-gpus</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/07/09/scaling-up-longer-video-generation-without-more-gpus.html">&lt;h1 id=&quot;scaling-up-longer-video-generation-model-training-without-more-gpus&quot;&gt;Scaling Up Longer Video Generation Model Training Without More GPUs&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/darth_vader_final.gif&quot; alt=&quot;darth_vader_final.gif&quot; /&gt;
&lt;img src=&quot;/assets/harley_final.gif&quot; alt=&quot;harley_final.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gif is manually resized to 256x256 and heavy lossy compressed using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gifsicle -O3 --lossy=200 --resize 256x256&lt;/code&gt; for better blog network loading speed, originally trained at 512x512 and generates more than 181 frames&lt;/p&gt;

&lt;p&gt;However these gifs are more than 1MB each, so if you have trouble loading the gif, you may need to go and download from github blog repo yourself, I can’t compress the gifs any further&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The dataset and model heavy resemble real life human with personal identity such as faces and bodys, thus can not go opensource for legal concerns&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;I used my RTX 3090Ti and created a 24370 clips dataset and trained a model under 24GB vram limitation that is capable of generating hundreds of frames with some consistency to the first frame, but during this experiment I changed every possible thing mid-training so there is no solid proof of what I learnt except for it more or less works this way&lt;/p&gt;

&lt;h2 id=&quot;scaling-up-the-dataset&quot;&gt;Scaling up the dataset&lt;/h2&gt;

&lt;p&gt;Last time I hand crafted a walking on stage video dataset containing 2848 clips, and I trained on each first 65 frames&lt;/p&gt;

&lt;p&gt;Which is bigger than the far previous 286 timelapse video dataset, but still too small for some real challenge&lt;/p&gt;

&lt;p&gt;So I gathered a human dancing dataset from various internet sources, containing 24370 video clips and has 181 frames each&lt;/p&gt;

&lt;p&gt;It is the most difficult subject for image generation and video generation: human and rapid motion&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The clips are aligned using pose detection, and resized to 512x512&lt;/li&gt;
  &lt;li&gt;Each clip contains at maximum 2 alternative augmentation, so there are more than 24370 actual clips when training&lt;/li&gt;
  &lt;li&gt;Contains some “bad” clips which contains heavy camera motion, or the human ran out of screen&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;scaling-up-video-duration-by-interpolation-and-extrapolation&quot;&gt;Scaling up video duration by interpolation and extrapolation&lt;/h2&gt;

&lt;p&gt;Last time I did video interpolation on the whole clip, which contains two interpolation stages: 5 frames –&amp;gt; 17 frames –&amp;gt; 65 frames&lt;/p&gt;

&lt;p&gt;And using local attention to crop down computational requirements&lt;/p&gt;

&lt;p&gt;Although it is working at least, but generating 65 frames already consumed 24GB vram even with accelerate/deepspeed optimization and gradient checkpointing&lt;/p&gt;

&lt;p&gt;If to generate as long as 181 frames, I decided to train the model in a autoregressive way&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;a base model generating 4 frames, and with some hacky inference technique, can generate 7 frames, as called “starter model”&lt;/li&gt;
  &lt;li&gt;a extrapolation model generate new 3 frames from the previous frames, but with a step of every 4th frames&lt;/li&gt;
  &lt;li&gt;a interpolation model fills the previously newly generated 3 frames with a total of 9 frames (fill in 3 frames into the two gaps)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The frame number generated in the following way (newly generated frame ends with a !):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1, 2!, 3!, 4!, 5!, 6!, 7!&lt;/li&gt;
  &lt;li&gt;1, 2, 3, 7, 11!, 15!, 19!&lt;/li&gt;
  &lt;li&gt;1, 2, 6, 7, 8!, 9!, 10!, 11 …&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I know it’s vague, don’t get too serious about it, it is a rough hack by myself and does not work too well, for now&lt;/p&gt;

&lt;p&gt;Good thing is that by this method, I don’t need to do gradient checkpointing and cpu offloading, which speeds up training further, not to mention 7 frames iters far quicker every step than 65 frames&lt;/p&gt;

&lt;p&gt;However, when handling dataset this large, I need to further speed it up, not only on the training side&lt;/p&gt;

&lt;h2 id=&quot;dataset-hack--cheating&quot;&gt;Dataset hack &amp;amp; cheating&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Well, if you are doing academical reserch, don’t do anything like this&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I got inspired by &lt;a href=&quot;https://arxiv.org/abs/2206.07137&quot;&gt;https://arxiv.org/abs/2206.07137&lt;/a&gt;, as the title suggests:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I decided to train the model on the full dataset first (and never ending), for a few epochs, then determine which data points are worth learning&lt;/p&gt;

&lt;p&gt;I don’t want to talk about it in detail, changing the dataset itself mid-training is a forbidden method because the result of the training became unreproducible and fragile&lt;/p&gt;

&lt;p&gt;However since I never finish training or writing papers, so this is not a problem to me&lt;/p&gt;

&lt;p&gt;I deleted 10% of the dataset which seems to be hard to learn in the first starter stage&lt;/p&gt;

&lt;p&gt;And also 10% in the expolation skipper stage, too, but not the same 10% (yeah, maybe it would be better if treated the same)&lt;/p&gt;

&lt;p&gt;The training loss drop like crazy, and the test generation is improving faster after since&lt;/p&gt;

&lt;p&gt;However, I can’t give out any proof of what I felt, this experiment is totally inaccurate because of the following reasons&lt;/p&gt;

&lt;h2 id=&quot;noise-augmentation-to-keep-the-long-time-consistency&quot;&gt;Noise augmentation to keep the long time consistency&lt;/h2&gt;

&lt;p&gt;When doing autoregressive generation, error gathers every iteration, so say we generate 120 frames in my method, we need to generate around 10 loops, each loop using previously generated content as hint, and that could be very inaccurate&lt;/p&gt;

&lt;p&gt;So firstly I employ signal noise ratio 0.33 at generated frames (but not the first hint frame), it seems to be good when testing with very few test clips&lt;/p&gt;

&lt;p&gt;Then I found it wasn’t enough, then I changed the augmentation noise, from signal noise ratio 0.33 to 0.66, it gets better, I feel better, no proof of any kind however …&lt;/p&gt;

&lt;p&gt;And this means I changed the augmentation mid-training, I would be fired if I am a scientist LOL&lt;/p&gt;

&lt;h2 id=&quot;half-way-fix-of-half-way-attention&quot;&gt;half-way fix of half-way attention&lt;/h2&gt;

&lt;p&gt;When I coded the first version of this experiment, I used a half-way attention to split the sequence in half then combine after calculation&lt;/p&gt;

&lt;p&gt;Which yields max 0.06 error every time and the average error is 0.01, I thought that was acceptable, much better than out of vram doing nothing&lt;/p&gt;

&lt;p&gt;But yet I forgot about it, and didn’t revert the half-way attention hack, when I realized about this, I decided to revert to ‘correct as a whole’ attention mid-training&lt;/p&gt;

&lt;p&gt;Okay, this is to say, I changed the model structure at mid training, this is not good, very not good, but neccesary&lt;/p&gt;

&lt;h2 id=&quot;power-failure-and-forgot-to-dump-adam-optimizer-state&quot;&gt;Power failure and forgot to dump adam optimizer state&lt;/h2&gt;

&lt;p&gt;Em… yeah, I forgot to dump adam optimizer state at first, then my apartment got power failure mid-training&lt;/p&gt;

&lt;p&gt;So, the training does not need to restart from the beginning but the training loss went crazy for days before it talks sense&lt;/p&gt;

&lt;h2 id=&quot;what-i-learned-from-the-experiment&quot;&gt;What I learned from the experiment&lt;/h2&gt;

&lt;p&gt;So much for confession, despite all the bad things I hacked and fixed, I actually learned something as follows&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Always dump optimizer states when training with adam something&lt;/li&gt;
  &lt;li&gt;A hack can be helpful at first when testing, if you forget about it when scaling up, it could be a disaster&lt;/li&gt;
  &lt;li&gt;Noise augmentation is very cool, but determine how much noise to add, is a total pain in the (beep)&lt;/li&gt;
  &lt;li&gt;Autoregressive is good, saves vram, saves time, if you code it right, it will crash later than sooner&lt;/li&gt;
  &lt;li&gt;I realized I have to redo the experiment again with smarter generation schedule to make sure the quality won’t drop significantly across time, not to mention everything I did wrong&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;not-really-a-conclusion&quot;&gt;Not Really a Conclusion&lt;/h2&gt;

&lt;p&gt;I changed model structure, augmentation, dataset, and optimizer state mid-training, these are unforgivable mistakes that should be avoided, but&lt;/p&gt;

&lt;p&gt;At least it works, barely works, but it works&lt;/p&gt;

&lt;p&gt;And hey, it’s under 24GB vram, and capable of generating hundreds of frames&lt;/p&gt;

&lt;p&gt;I am so eager to share with everyone what I did good, but currently the quality is poor, that is to say I am not doing good for now&lt;/p&gt;

&lt;p&gt;So at it’s current state, if to claim that the model works, it would be a false claim, sharing non-working code would be irresponsible and thus I won’t update my github repo this time, but hopefully not for long&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Every time the generated illustrated figure tries to turn their heads left or right, it creates artifacts, stable diffusion v1.5 cannot handle these circumstances well&lt;/li&gt;
  &lt;li&gt;The generated figure tends to become female in the autoregressive pipeline, due to the dataset bias&lt;/li&gt;
  &lt;li&gt;Although in theory it can generate unlimited length of clips, human rapid actions always reach a status that the generation is broken, such as too far or too close to the camera etc&lt;/li&gt;
  &lt;li&gt;If the generated figure not moving fast, there is overfitting on background&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;Thanks to the opensource repos made by &lt;a href=&quot;https://github.com/lucidrains&quot;&gt;https://github.com/lucidrains&lt;/a&gt; , including but not limited to:&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/video-diffusion-pytorch&lt;/p&gt;

&lt;p&gt;And my code is based on &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;https://github.com/huggingface/diffusers&lt;/a&gt;, especially most of the speed up tricks are bundled within the original repository&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{mindermann2022prioritized,
      title={Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt}, 
      author={Sören Mindermann and Jan Brauner and Muhammed Razzak and Mrinank Sharma and Andreas Kirsch and Winnie Xu and Benedikt Höltgen and Aidan N. Gomez and Adrien Morisot and Sebastian Farquhar and Yarin Gal},
      year={2022},
      eprint={2206.07137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{Singer2022,
    author  = {Uriel Singer},
    url     = {https://makeavideo.studio/Make-A-Video.pdf}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Scaling Up Longer Video Generation Model Training Without More GPUs</summary></entry><entry><title type="html">在24gb显存下教育外国大模型替中国说话</title><link href="http://localhost:4000/jekyll/update/2023/03/10/finetune-flan-x5-xxl-under-24gb-vram.html" rel="alternate" type="text/html" title="在24gb显存下教育外国大模型替中国说话" /><published>2023-03-10T19:33:05+08:00</published><updated>2023-03-10T19:33:05+08:00</updated><id>http://localhost:4000/jekyll/update/2023/03/10/finetune-flan-x5-xxl-under-24gb-vram</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/03/10/finetune-flan-x5-xxl-under-24gb-vram.html">&lt;h1 id=&quot;在24gb显存下教育外国大模型替中国说话&quot;&gt;在24gb显存下教育外国大模型替中国说话&lt;/h1&gt;

&lt;p&gt;For english readers: this post is about “Finetune flan-t5-xxl under 24gb vram”, you can view &lt;a href=&quot;https://www.philschmid.de/fine-tune-flan-t5-deepspeed&quot;&gt;philschmid tutorial&lt;/a&gt; and find my code and model at &lt;a href=&quot;https://www.modelscope.cn/models/lxj616/flan-t5-xxl-lora-chinese-spokesman&quot;&gt;modelscope&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;模型和代码均位于&lt;a href=&quot;https://www.modelscope.cn/models/lxj616/flan-t5-xxl-lora-chinese-spokesman&quot;&gt;modelscope&lt;/a&gt;, 嫌麻烦的话直接去运行py脚本就好了&lt;/p&gt;

&lt;h2 id=&quot;未经开化的外国模型不识大体&quot;&gt;未经开化的外国模型不识大体&lt;/h2&gt;

&lt;p&gt;For english readers: model without finetune is unfamiliar with specific downstream tasks, generation is of poor quality&lt;/p&gt;

&lt;p&gt;开源的模型虽然好，就像别人家的孩子一样，没有自己家的孩子听话，不懂替自己家说话&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/compare_lora_005.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以本文介绍了，如何在24GB显存家用电脑上，教育国外模型替中国说话&lt;/p&gt;

&lt;h2 id=&quot;环境搭建&quot;&gt;环境搭建&lt;/h2&gt;

&lt;p&gt;For english readers: use Dockerfile to set up training envionment&lt;/p&gt;

&lt;p&gt;我已经写好Dockerfile了，直接一步到位，而且还切换了国内源，国内连接不上的nltk data直接本地拷贝进去的，一行命令就解决环境搭建了&lt;/p&gt;

&lt;h2 id=&quot;实验数据集&quot;&gt;实验数据集&lt;/h2&gt;

&lt;p&gt;For english readers: dataset is located at modelscope as well&lt;/p&gt;

&lt;p&gt;《外交部发言人答记者问》数据集也位于modelscope，训练集包含了10000个问答，测试集是凑整数剩下的尾巴，里面都是外交部官方公布的数据，绝对合规&lt;/p&gt;

&lt;p&gt;而且因为都是中文的，就没放github而是modelscope，也是出于合规考虑，中国的东西放在国内&lt;/p&gt;

&lt;p&gt;数据集已经翻译成英文了，毕竟外交部答记者问一般是给歪果仁看的，训练起来也方便，在QQ机器人里对话时自动使用谷歌翻译来进行对话&lt;/p&gt;

&lt;h2 id=&quot;训练&quot;&gt;训练&lt;/h2&gt;

&lt;p&gt;For english readers: training the model is as simple as running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_transformer.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果环境搭建好了，直接运行&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_transformer.py&lt;/code&gt;就好了，训练之前你可以自己改自己想用的数据集，注意这里用的基础模型flan-t5-xxl是需要预先下载到本地的，在脚本中自己改本地设置，这个模型非常巨大（删完没用的还有209G），下载前做好心理准备，或者干脆手动下载pytorch的把其他的都扔了&lt;/p&gt;

&lt;p&gt;训练需要至少24GB，其他可选参数见脚本，如果有疑问，翻文章上面philschmid的教程看好了&lt;/p&gt;

&lt;h2 id=&quot;生成&quot;&gt;生成&lt;/h2&gt;

&lt;p&gt;For english readers: inference is as simple as running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_peft.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;而想要使用这个模型，可以直接去运行&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference_peft.py&lt;/code&gt;，注意，如果想要运行优化后的模型，基础模型你也是需要下载好的，需要到huggingface下载209GB的flan-t5-xxl，我默认是本地加载，因为每次生成或者训练都重新网络加载那么巨大的模型是不现实的，还不如放在本地再试图运行&lt;/p&gt;

&lt;h2 id=&quot;学习后看看学傻了没&quot;&gt;学习后看看学傻了没&lt;/h2&gt;

&lt;p&gt;For english readers: after finetuning the model, check if the model can generalize on previous unrelated tasks&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/compare_lora_004.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;嘿！没想到这学习完替中国说话，这模型竟然把之前答错的算术题给答对咯，看来学习外交部发言人的答记者问有助于模型提高数学智商～&lt;/p&gt;

&lt;h2 id=&quot;qq机器人&quot;&gt;QQ机器人&lt;/h2&gt;

&lt;p&gt;For english readers: this is a chatbot implementation that is related to a not-open-source-software&lt;/p&gt;

&lt;p&gt;一个QQ机器人的示例代码稍后会更新到modelscope那边，不保证能跑，反正我自己能跑就行&lt;/p&gt;

&lt;h2 id=&quot;其他例子&quot;&gt;其他例子&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/compare_lora_003.jpg&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/assets/compare_lora_002.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">在24gb显存下教育外国大模型替中国说话</summary></entry><entry><title type="html">Make a longer stable diffusion video on home computers</title><link href="http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html" rel="alternate" type="text/html" title="Make a longer stable diffusion video on home computers" /><published>2023-03-05T09:37:50+08:00</published><updated>2023-03-05T09:37:50+08:00</updated><id>http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html">&lt;h1 id=&quot;make-a-longer-stable-diffusion-video-on-home-computers&quot;&gt;Make A Longer Stable Diffusion Video On Home Computers&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/trump_small.gif&quot; alt=&quot;trump_small.gif&quot; /&gt;
&lt;img src=&quot;/assets/spiderman3_small.gif&quot; alt=&quot;spiderman3_small.gif&quot; /&gt;
&lt;img src=&quot;/assets/ironman_small.gif&quot; alt=&quot;ironman_small.gif&quot; /&gt;
&lt;img src=&quot;/assets/spiderman2_small.gif&quot; alt=&quot;spiderman2_small.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gif is manually resized to 256x256 and heavy lossy compressed using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gifsicle -O3 --lossy=200 --resize 256x256&lt;/code&gt; for better blog network loading speed, originally trained at 512x512 and 64 frames&lt;/p&gt;

&lt;p&gt;However these gifs are more than 1MB each, so if you have trouble loading the gif, you may need to go and download from github blog repo yourself, I can’t compress the gifs any further&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The dataset and model heavy resemble real life human with personal identity such as faces and bodys, thus can not go opensource for legal concerns&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;longer-problem-for-longer-video&quot;&gt;Longer Problem For Longer Video&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;It simply just won’t work if you set frames_length to higher value and press enter harder with your finger&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;My previous timelapse toy model used timelapse video dataset, although they have adequate clip length for longer video experiment, it doesn’t make sense training longer sequence when shorter is good enough&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As in tradition, one single RTX 3090Ti (24 GB vram) is what I got, and all the fancy longer video generation stuff, I mean it to get it done with the exact same home computer computation limitations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-missing-make-a-video-technique&quot;&gt;The Missing Make A Video Technique&lt;/h2&gt;

&lt;p&gt;Well, I already give out make-a-stable-diffusion-video github repo to demonstrate how to make it work, especially on home computers&lt;/p&gt;

&lt;p&gt;And I stated that in my last blog post: ‘Oh, I can not afford that with 24gb vram, let’s just pretend there isn’t a whole paragraph in make-a-video paper explaining video frame interpolation’&lt;/p&gt;

&lt;p&gt;Now that I’m gonna try more frames and wish to get coherent results for a long range of frames, and probably finish training myself instead of leaving a letter to my grandson&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Video frame interpolation is what I need, the missing piece&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/task/video-frame-interpolation&quot;&gt;https://paperswithcode.com/task/video-frame-interpolation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So I made a hack to my code, using the inpainting model special feature to implement fast and incorrect interpolation&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hint_latents = latents[:,:,0::4,:,:]
hint_latents_expand = hint_latents.repeat_interleave(4,2)
hint_latents_expand = hint_latents_expand[:,:,:args.frames_length,:,:]
latent_model_input = torch.cat([noisy_latents, masks_input, hint_latents_expand], dim=1).to(accelerator.device)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Well, for every 4 frames, set the original frame to inpainting condition input to generate exact frame image since I masked nothing&lt;/p&gt;

&lt;p&gt;Good thing is this hack is almost one line without custom attention module modification, bad thing is this is mathematically wrong because I really should set the static frame without model backbone inference on it&lt;/p&gt;

&lt;p&gt;And as for duplicating the hint input to every subordinate frames, I didn’t do research on its effects, I can’t answer it because I have no clue myself&lt;/p&gt;

&lt;p&gt;So, here is the plan:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;generate 5 frames&lt;/li&gt;
  &lt;li&gt;interpolate to 17 frames, (5-1)x4+1=17&lt;/li&gt;
  &lt;li&gt;interpolate to 65 frames, (17-1)x4+1=65&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;attention-is-all-i-can-not-afford&quot;&gt;Attention Is All I Can Not Afford&lt;/h2&gt;

&lt;p&gt;Surely I wouldn’t meet many trouble dealing with 5 frames, whatever attention I use&lt;/p&gt;

&lt;p&gt;But 65 frames leads to a huge problem, especially when we do interpolation&lt;/p&gt;

&lt;p&gt;Normal attention has O(n^2) complexity&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For 5 frames, n is 5, so that would be 5^2=25 units of complexity&lt;/strong&gt;
&lt;strong&gt;For 65 frames, n is 65, so that would be 65^2=4225 units of complexity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So it is obvious I need to train 65 frames model 169x times than the 5 frames model, this could be a problem&lt;/p&gt;

&lt;p&gt;I actually tried that for comparison, the 65 frames model is a total mess visually and by loss curve, even already trained for 20 epochs, I should have saved the screenshot, but I get too frustrated and forgot&lt;/p&gt;

&lt;p&gt;Here comes a better(?) attention mechanism for long sequences&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/method/sliding-window-attention&quot;&gt;https://paperswithcode.com/method/sliding-window-attention&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With sliding window attention such as local attention, you only need O(n x w) complexity, for 65 frames and windowsize 5, its 65x5=325 units of complexity, compared to 4225 it almost seem like a silver bullet&lt;/p&gt;

&lt;p&gt;But, does it ?&lt;/p&gt;

&lt;p&gt;By using &lt;a href=&quot;https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/local.py&quot;&gt;https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/local.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I found the shortcomings of local attention, which is too narrow minded on adjacent frames, the most notable effect is the identity of the main subject changes rapidly across frames&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Good thing is local attention’s shortcomings are trival to our interpolation network, we always have a reference frame every 4th frames, so no need to worry about identity change&lt;/p&gt;

&lt;p&gt;Note: I have no idea what windowsize is the best, I just used 5 which is the default value&lt;/p&gt;

&lt;p&gt;It turned out by using local attention, I am being able to train the 65 frames interpolation network for 20 epochs and seems to be good enough (my fancy way of saying I ran out of patience and stopped)&lt;/p&gt;

&lt;h2 id=&quot;a-not-too-short-and-not-too-small-dataset-for-testing&quot;&gt;A Not Too Short And Not Too Small Dataset For Testing&lt;/h2&gt;

&lt;p&gt;Many would ask: what’s wrong with the timelapse video dataset, it’s long enough and you already have that, why bother making another dataset ?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;timelapse video dataset is too small (286 videos), it does not generalize well to make creativity art, a cat standing idle with clouds moving is almost the only thing it does&lt;/li&gt;
  &lt;li&gt;stable diffusion is good at generating landscape images, and human eyes are not sensitive about nature scenes, nature landscape change very little across time, a timelapse dataset can cover hidden technical problems, but this time I shall face the real challenge&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, this time I made a ‘fashion model walking on stage’ video dataset, it has the following features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;2848 videos, all above 100 frames, almost 10x the size of timelapse video dataset&lt;/li&gt;
  &lt;li&gt;contains human, a forbidden area for stable diffusion both for poor generation quality and for legal obligations&lt;/li&gt;
  &lt;li&gt;human changes scale across time, far to near, not the same size&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Speak of legal obligations, I am not being able to opensource the dataset or the model, because it is trained on human, it has to more or less contain personal identity information such as faces and bodys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All I could publish is where I got the raw videos:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/@yaersfashiontv&quot;&gt;https://www.youtube.com/@yaersfashiontv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And I used blender to preprocess the videos, manually&lt;/p&gt;

&lt;h2 id=&quot;unverified-experimental-hacks&quot;&gt;Unverified experimental hacks&lt;/h2&gt;

&lt;p&gt;Except for what I did and tested above, I actually experimented custom attention patterns like always attend to first frame no matter what window size&lt;/p&gt;

&lt;p&gt;But I can not tell the difference, so without proof I can just say I am not able to confirm whether they work or not&lt;/p&gt;

&lt;h2 id=&quot;recommended-opensource-implementations&quot;&gt;Recommended Opensource Implementations&lt;/h2&gt;

&lt;p&gt;I have noticed that there is a cleaner and easier to use implementation other than my make-a-stable-diffusion-video repo&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you got enough vram and wish not to use my hacks (which mainly focus on running under 24GB vram), you can check this work in progress implementation by chavinlo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/chavinlo/TempoFunk&quot;&gt;https://github.com/chavinlo/TempoFunk&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;Thanks to the opensource repos made by &lt;a href=&quot;https://github.com/lucidrains&quot;&gt;https://github.com/lucidrains&lt;/a&gt; , including but not limited to:&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/video-diffusion-pytorch&lt;/p&gt;

&lt;p&gt;And my code is based on &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;https://github.com/huggingface/diffusers&lt;/a&gt;, especially most of the speed up tricks are bundled within the original repository&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{Singer2022,
    author  = {Uriel Singer},
    url     = {https://makeavideo.studio/Make-A-Video.pdf}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{ho2022video,
  title   = {Video Diffusion Models}, 
  author  = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  year    = {2022},
  eprint  = {2204.03458},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Make A Longer Stable Diffusion Video On Home Computers</summary></entry><entry><title type="html">Make a stable diffusion video on home computers</title><link href="http://localhost:4000/jekyll/update/2022/12/26/make-a-stable-diffusion-video-on-home-computers.html" rel="alternate" type="text/html" title="Make a stable diffusion video on home computers" /><published>2022-12-26T13:14:58+08:00</published><updated>2022-12-26T13:14:58+08:00</updated><id>http://localhost:4000/jekyll/update/2022/12/26/make-a-stable-diffusion-video-on-home-computers</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/12/26/make-a-stable-diffusion-video-on-home-computers.html">&lt;h1 id=&quot;make-a-stable-diffusion-video-on-home-computers&quot;&gt;Make A Stable Diffusion Video On Home Computers&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/gif_cat_small.gif&quot; alt=&quot;gif_cat_small.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The gif is manually resized to 256x256 and crop only 12 frames for better blog network loading speed, originally trained at 512x512 and 25 frames (can generate as much as 120 frames when inference)&lt;/p&gt;

&lt;h2 id=&quot;now-share-the-working-code&quot;&gt;Now share the working code&lt;/h2&gt;

&lt;p&gt;In my last post about adding pseudo-3d structure to stable-diffusion model, I did not share the code simply because I can not prove it working due to lack of compute, and I already explained that in my last post&lt;/p&gt;

&lt;p&gt;Now I get it working and I trained a toy model with my single RTX 3090Ti, making the theory into real practice, and now it is time for sharing the code&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lxj616/make-a-stable-diffusion-video&quot;&gt;https://github.com/lxj616/make-a-stable-diffusion-video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And a pretrained toy model is at &lt;a href=&quot;https://huggingface.co/lxj616/make-a-stable-diffusion-video-timelapse&quot;&gt;huggingface&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;further-improve-the-vram-consumption--training-speed&quot;&gt;Further improve the vram consumption &amp;amp; training speed&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Finally I gave up using the full precision stable diffusion backbone, in my last post I tried to do mix precision training and enable fp16 only on new layers, now I regret that&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://github.com/huggingface/accelerate&quot;&gt;https://github.com/huggingface/accelerate&lt;/a&gt; to offload optimizer states to cpu could spare more vram&lt;/li&gt;
  &lt;li&gt;Freeze the backbone and filter the parameters for optimization, proved successful despite the make-a-video paper suggests training altogether (maybe for better generation quality? I can not afford that)&lt;/li&gt;
  &lt;li&gt;Because of switching to diffusers repository and met some problems with (xformers + RTX 3090ti), used custom build flash attention as alternative, it’s faster compared to not enable xformers&lt;/li&gt;
  &lt;li&gt;Did a partial gradient checkpointing trick to make training a little faster when vram has spared some space with half precision&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Turns out I made a huge mistake in my last post, I thought fp16 is to be used on new layers with mixed precision training like all tutorials suggests, but afterwards I realized to save more vram actually is done mainly by reduce the backbone precision, thanks to the diffuers repository, I instantly realized a half accurate model is better than totally unusable&lt;/p&gt;

&lt;h2 id=&quot;a-much-smaller-dataset-for-quick-testing-and-toy-training&quot;&gt;A much smaller dataset for quick testing and toy training&lt;/h2&gt;

&lt;p&gt;Another huge mistake I made in my last post is to build a 3970 size driving video dataset, which took forever to train on a single RTX 3090Ti, it’s still too large for testing&lt;/p&gt;

&lt;p&gt;I was so confused why the plants are not moving backward as we drive, the reason is simple: not enough training and not enough frames length&lt;/p&gt;

&lt;p&gt;The timelapse video dataset contains only 286 videos, so that I can easily get 60 epochs in hours, with a much better optimized(sort of overfit) training output&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/loss_video_diffusers.jpg&quot; alt=&quot;loss_video_diffusers.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you remember, the loss from my last post is higher than 0.105 and now is down to 0.08, not to mention the increased frames length give more stability&lt;/p&gt;

&lt;p&gt;This timelapse dataset is mainly come from &lt;a href=&quot;http://www.setvak.cz/setvak-cz.html&quot;&gt;Martin Setvak&lt;/a&gt; and others, trained using frames_length=25 and fp16 (some experiment using bfloat16, I may not remember accurately, definitely not full precision for sure)&lt;/p&gt;

&lt;p&gt;If you need this dataset, you can download from Martin Setvak website yourself because I am not allowed to redistribute, and the original author can choose to cancel sharing whenever he likes&lt;/p&gt;

&lt;p&gt;Due to some frozen seconds at the beginning of the video, I crop the video using following script, as well as crop to 512x512 and reduce framerate to 5&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#!/bin/bash
for i in `ls matin_setvak_video/`
do
    mkdir -p matin_setvak_frames/$i
    duration=`ffprobe -v error -show_entries format=duration -of csv=p=0 &quot;matin_setvak_video/$i&quot;`
    cut_duration=`python -c &quot;print($duration - 4.0)&quot;`
    ffmpeg -i &quot;matin_setvak_video/$i&quot; -ss 00:00:02.00 -t $cut_duration -r 5 -y -an -q 0 -vf scale=&quot;'if(gt(iw,ih),-1,512):if(gt(iw,ih),512,-1)', crop=512:512:exact=1&quot; matin_setvak_frames/$i/%06d.jpg
    rm matin_setvak_frames/$i/000001.jpg
done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;does-it-work-good-enough&quot;&gt;Does it work good enough&lt;/h2&gt;

&lt;p&gt;Well, much better than last time, but I got to admit that 286 video is disastrously small and of very little use for video generation&lt;/p&gt;

&lt;p&gt;All the toy model do is to generate moving clouds and timelapse lighting progressing acoss the landscape, it literally only does timelapse&lt;/p&gt;

&lt;p&gt;The model was never trained on cats and so the cat does not move at all, and clouds is the only thing it can generate, LOL&lt;/p&gt;

&lt;p&gt;But I believe it works, at least the clouds are moving and the cat is not, and the cat is getting shifting lighting across time just like landscape, yay ~&lt;/p&gt;

&lt;p&gt;With the code actually do something, there is no telling if I accidentaly did something wrong, there still be open possibilities, I don’t give any warrant on the sharing code, okay&lt;/p&gt;

&lt;h2 id=&quot;further-plan&quot;&gt;Further plan&lt;/h2&gt;

&lt;p&gt;If not any new models come out proving to be more efficient or better looking, I could try getting larger dataset training with limited computing resources, but for my current compute capacity, I would not dream of trying anything larger than 3000 videos&lt;/p&gt;

&lt;p&gt;So my best bet is something like stable diffusion for video go opensource and finetune on that&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;Thanks to the opensource repos made by &lt;a href=&quot;https://github.com/lucidrains&quot;&gt;https://github.com/lucidrains&lt;/a&gt; , including but not limited to:&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/video-diffusion-pytorch&lt;/p&gt;

&lt;p&gt;And my code is based on &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;https://github.com/huggingface/diffusers&lt;/a&gt;, especially most of the speed up tricks are bundled within the original repository&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{Singer2022,
    author  = {Uriel Singer},
    url     = {https://makeavideo.studio/Make-A-Video.pdf}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{ho2022video,
  title   = {Video Diffusion Models}, 
  author  = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  year    = {2022},
  eprint  = {2204.03458},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Make A Stable Diffusion Video On Home Computers</summary></entry><entry><title type="html">Make a stable diffusion video with temporal attention conv layers</title><link href="http://localhost:4000/jekyll/update/2022/11/15/make-a-stable-diffusion-video-with-temporal-attention-conv-layers.html" rel="alternate" type="text/html" title="Make a stable diffusion video with temporal attention conv layers" /><published>2022-11-15T13:23:01+08:00</published><updated>2022-11-15T13:23:01+08:00</updated><id>http://localhost:4000/jekyll/update/2022/11/15/make-a-stable-diffusion-video-with-temporal-attention-conv-layers</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/11/15/make-a-stable-diffusion-video-with-temporal-attention-conv-layers.html">&lt;h1 id=&quot;make-a-stable-diffusion-video-with-temporal-attention--conv-layers&quot;&gt;Make A Stable Diffusion Video With Temporal Attention &amp;amp; Conv Layers&lt;/h1&gt;

&lt;p&gt;Long story short, I put &lt;a href=&quot;https://github.com/lucidrains/make-a-video-pytorch&quot;&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/a&gt; into &lt;a href=&quot;https://github.com/CompVis/stable-diffusion/&quot;&gt;https://github.com/CompVis/stable-diffusion/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/make-a-stable-diffusion-video.jpg&quot; alt=&quot;make-a-stable-diffusion-video.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The prompts are (top to bottom):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cyberpunk imaginary scenic byway road drive, trending on artstation&lt;/li&gt;
  &lt;li&gt;volcano fire burning scenic byway road drive, trending on artstation&lt;/li&gt;
  &lt;li&gt;summer dawn scenic byway road drive, award winning photography&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Trained with a 4 hour driving video&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ZOZOqbK86t0&quot;&gt;4K Scenic Byway 12 All American Road in Utah, USA - 5 Hour of Road Drive with Relaxing Music&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;split into frames using a low framerate (-r 10)&lt;/p&gt;

&lt;p&gt;The video is random selected and from &lt;a href=&quot;https://www.youtube.com/c/RelaxationChannel&quot;&gt;RelaxationChannel&lt;/a&gt; , and crop center 512x512&lt;/p&gt;

&lt;h2 id=&quot;a-video-in-theory-is-not-a-video-with-real-reasonable-quality&quot;&gt;A video in theory is not a video with real reasonable quality&lt;/h2&gt;

&lt;p&gt;If you take a deeper look, it’s not hard to spot these image sequence are all “driving video” with poor consistency across time, to name a few:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The road is changing fast across time because we are driving, good, the far background isn’t moving as fast because they are far away, cool, but how the hell the plants are not moving backwards as we drive ?&lt;/li&gt;
  &lt;li&gt;A video needs at least several seconds maybe, how come 5 images be called a video ?&lt;/li&gt;
  &lt;li&gt;A model that only generate “driving video” is not a text to video model, it’s more like a text based style filter for very specific reference video, could it generalize as text to video with current proof ?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;computational-impossible-for-home-computers&quot;&gt;Computational impossible for home computers&lt;/h2&gt;

&lt;p&gt;The original stable-diffusion Unet model has around &lt;strong&gt;859.52 M parameters&lt;/strong&gt;, and is said to use 4000 gpus a month for v1.4 let alone further versions&lt;/p&gt;

&lt;p&gt;When extending the original stable diffusion model with temporal attention/conv layers, &lt;strong&gt;it reached 1100 M params&lt;/strong&gt;, and is dealing with multi-frame image data compared to original single image processing&lt;/p&gt;

&lt;p&gt;And &lt;strong&gt;I got one single 24G vram RTX 3090Ti&lt;/strong&gt;, even if it could somehow fit in the vram, there’s definitely no way getting another 3999 gpus, or experimenting a whole month&lt;/p&gt;

&lt;p&gt;Thus I am writting a blog with poor results knowing it is not nearly done training, it can not be done with my current computing capability&lt;/p&gt;

&lt;h2 id=&quot;insert-11b-model-elephant-into-24g-vram-refrigerator-and-go-training&quot;&gt;Insert 1.1B model elephant into 24G vram refrigerator and go training&lt;/h2&gt;

&lt;p&gt;I’ll just list the hacks I am using, we would not discuss so many papers/hacks all day&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;flash-attention&lt;/a&gt; with f16, and memory efficient normal attention&lt;/li&gt;
  &lt;li&gt;Remove first-stage-model and cond-stage-model, pre-compute the embeddings (see my previous post)&lt;/li&gt;
  &lt;li&gt;Let’s call 5 frames a video, especially when I can not afford more frames&lt;/li&gt;
  &lt;li&gt;Freeze the original attention layers, and not conditioning on text in temporal attention&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And some other hacks does not seem to work, list below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;8bit Adam seems to spare 40MB more vram, quite trival for my case, maybe I am not deploying right ?&lt;/li&gt;
  &lt;li&gt;I can not cut down model channels because stable diffusion backbone requires exact 320 channels&lt;/li&gt;
  &lt;li&gt;Try image first and video later so as to freeze the conv layers, but it does not work, I’ll try it again later&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Finally make it going with a batch size of 2 each gpu node, and I got one gpu, so that’s batch 2 every step, for 4 days&lt;/p&gt;

&lt;p&gt;stable diffusion used batch 2048 and 1400k steps, I got batch 2 and 657k steps, and I’m dealing with a much bigger 1.1B param model, and video !&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/loss_make_a_stable_diffusion_video.png&quot; alt=&quot;loss_make_a_stable_diffusion_video.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After 4 days of training, the loss is still observable decreasing and actually generation models does not stop on loss converge, not to mention it’s far from converge&lt;/p&gt;

&lt;p&gt;But I can wait no more, at this rate &lt;strong&gt;I’ll need to wait around 4 x (2048 x 1400k)/(2 x 657k) / 365 ~= 20 years&lt;/strong&gt;, lets stop here at where it is&lt;/p&gt;

&lt;h2 id=&quot;plants-not-moving-backwards-as-we-drive&quot;&gt;Plants not moving backwards as we drive&lt;/h2&gt;

&lt;p&gt;At first I suspect my freezing layers disrupt the learning process, maybe train a latent video diffusion could have made the plants looks like going backwards when driving ?&lt;/p&gt;

&lt;p&gt;I combined &lt;a href=&quot;https://github.com/lucidrains/video-diffusion-pytorch&quot;&gt;https://github.com/lucidrains/video-diffusion-pytorch&lt;/a&gt; with stable diffusion f8 first stage model, and trained from scratch&lt;/p&gt;

&lt;p&gt;But only to notice the same problem, the road is moving, the sky and clouds are minimum shifting a little, the trees however are changing shape like horror movies&lt;/p&gt;

&lt;p&gt;I’m out of ideas, so be it&lt;/p&gt;

&lt;h2 id=&quot;more-frames-at-inference-time&quot;&gt;More frames at inference time&lt;/h2&gt;

&lt;p&gt;Yes, I can generate 6 frames when training at 5&lt;/p&gt;

&lt;p&gt;But I have doubts if training only 5 frames could leverage the temporal attention capabilities well, that is too many parameters for too short sequence&lt;/p&gt;

&lt;p&gt;And Make-A-Video implements a frame interpolation mechanism, and multi framerate training, I could not do that, neither adding more layers nor conditioning on framerate is 24G vram friendly, if I decrease the batchsize to 1, I may need 40 years to do experiment on it&lt;/p&gt;

&lt;p&gt;I’m out of gpus, so be it&lt;/p&gt;

&lt;h2 id=&quot;text-to-driving-video-evaluation&quot;&gt;Text to driving video evaluation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;It can utilize the original stable diffusion attention, and discover volcano/cyberpunk/dawn styles and object like volcanos, manipulation with text&lt;/li&gt;
  &lt;li&gt;It’s trying to generate consistent frames across time, and deal with roads/sideway buildings/far-horizon-objects differently, however it does very very bad due to low compute, and low compute is not the only reason, object come in closer is something hard for the model to understand&lt;/li&gt;
  &lt;li&gt;I used to thought maybe utilizing the stable diffusion model and adding temporal layers could be easy piece, but now the 1.1B model isn’t vram friendly at all, this is so hard&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;keep-going-and-careful-plan-sharing-unverified-code&quot;&gt;Keep going and careful plan sharing unverified code&lt;/h2&gt;

&lt;p&gt;It would be nice if I make a totally working make-a-stable-diffusion-video open source repository and sharing with others, but now the fact is that I can not conclude this is working correctly and I can not finish training&lt;/p&gt;

&lt;p&gt;I’d be cautious and try testing it with limited computing resources, however others may release more powerful research or pretrained network structure models soon, it would be much better if I can finetune on something instead of doing from scratch myself&lt;/p&gt;

&lt;p&gt;Not until I make something really convincing, I would not do a fraud repo containing unverified code, the “volcano fire burning scenic byway road drive” may seem to be working, but it’s not science nor art this way, yet&lt;/p&gt;

&lt;p&gt;Keep going, to infinity and beyond&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;Thanks to the opensource repos made by &lt;a href=&quot;https://github.com/lucidrains&quot;&gt;https://github.com/lucidrains&lt;/a&gt; , including but not limited to:&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/p&gt;

&lt;p&gt;https://github.com/lucidrains/video-diffusion-pytorch&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{Singer2022,
    author  = {Uriel Singer},
    url     = {https://makeavideo.studio/Make-A-Video.pdf}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{ho2022video,
  title   = {Video Diffusion Models}, 
  author  = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  year    = {2022},
  eprint  = {2204.03458},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Make A Stable Diffusion Video With Temporal Attention &amp;amp; Conv Layers</summary></entry><entry><title type="html">Improve short video consistency with stable diffusion</title><link href="http://localhost:4000/jekyll/update/2022/09/29/improve-short-video-consistency-with-stable-diffusion.html" rel="alternate" type="text/html" title="Improve short video consistency with stable diffusion" /><published>2022-09-29T19:04:25+08:00</published><updated>2022-09-29T19:04:25+08:00</updated><id>http://localhost:4000/jekyll/update/2022/09/29/improve-short-video-consistency-with-stable-diffusion</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/09/29/improve-short-video-consistency-with-stable-diffusion.html">&lt;h1 id=&quot;improve-short-video-consistency-with-stable-diffusion&quot;&gt;Improve Short Video Consistency With Stable Diffusion&lt;/h1&gt;

&lt;p&gt;Stable diffusion has a built-in example for img2img generation and thus we could easily adopt it for vid2vid, however, it does not seem to be good enough keeping the video frames consistent and smooth&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/video_consistency.gif&quot; alt=&quot;video_consistency.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In case you have doubts, I already used fixed noise and fixed seed(s) for all frames, now we can focus on the obvious problems&lt;/p&gt;

&lt;p&gt;The gif above is resized and compressed for better webpage loading, not the original length and quality&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;One problem is that if we select a ‘noising strength’ too low as right-top-corner (–strength 0.45), the model seems doing trival edits which does not do anything but for adding jumping artifacts across frames&lt;/li&gt;
  &lt;li&gt;Another problem is that if we select a higher ‘noising strength’ as left-bottom-corner (–strength 0.75), the model ignores the obvious object across frames and makes the car disappear, and I still feel it not artistic enough&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here I adopt a idea from paper &lt;a href=&quot;https://arxiv.org/abs/2201.11632&quot;&gt;Deep Video Prior for Video Consistency and Propagation&lt;/a&gt;, and make it like right-bottom-corner achieving better video consistency for short videos&lt;/p&gt;

&lt;h2 id=&quot;not-the-old-content-style-balance-problem&quot;&gt;Not the old content style balance problem&lt;/h2&gt;

&lt;p&gt;If you remember the old neural style stuff, you could recall something named content style balance, there is a magic ratio to be tuned manually so as to find better trade-off on content fidelity against style&lt;/p&gt;

&lt;p&gt;Here we have a parameter ‘noising strength’, you put a 0.01 and got near exact the original content, and you put a 0.99 for total imagination with prompt, could there be a satisfying value in the middle ?&lt;/p&gt;

&lt;p&gt;Well, I couldn’t find one, and even with my hack done, the video is still kind of jumpy, the improvement is limited&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You have to increase content fidelity by using a lower noising strength for video frames consistency, but how are you going to make notable text prompt edits on such low noising strength ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now we got a problem to solve&lt;/p&gt;

&lt;h2 id=&quot;short-video-as-the-unconditional-dataset&quot;&gt;Short video as the unconditional dataset&lt;/h2&gt;

&lt;p&gt;We hope the stable diffusion model to generate video frames according to the reference video, at some degree, we do not wish to generate something far from all frames&lt;/p&gt;

&lt;p&gt;So we could finetune the stable diffusion model to reconstruct better video frames if not given any text prompts, then use text prompts to edit them&lt;/p&gt;

&lt;p&gt;A fun fact is that after many experiments, I found 30 frames is good enough to deal with a 300 frames short video, not really need to finetune on them all, unless your video got sudden subject twists&lt;/p&gt;

&lt;h2 id=&quot;text-to-image-as-the-conditional-dataset&quot;&gt;Text to image as the conditional dataset&lt;/h2&gt;

&lt;p&gt;Select a frame as a example, do txt2img until you are satisfied, with a rather large noising strength, don’t worry about the content may inconsistent with the original frame yet, we have more steps further down&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It is okay that the edited frame has obviously changed too much in color space, for example black shirts to red dress, you may use (–strength 0.75) and even more&lt;/li&gt;
  &lt;li&gt;It is NOT okay if the subject changed composition too much, for example human arm position may change a lot, generate more images to select the nearest one, or decrease the noising strength, frames are going jumpy otherwise&lt;/li&gt;
  &lt;li&gt;Remember the text prompt&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;finetune-on-combined-dataset&quot;&gt;Finetune on combined dataset&lt;/h2&gt;

&lt;p&gt;Now, we got a unconditional dataset which consists 30 frames, with empty text embedding, a conditional dataset consists maybe 2 different text prompts on 1 frame&lt;/p&gt;

&lt;p&gt;So we have a dataset of 32 frames in total&lt;/p&gt;

&lt;p&gt;Let’s resume training on stable diffusion as finetuning, if you have not read my previous post about how to finetune the model, it is time to go for it now&lt;/p&gt;

&lt;p&gt;I have also employed some techniques I discovered earlier, including only finetune on late steps to speed up training&lt;/p&gt;

&lt;p&gt;And make sure text conditional dataset start denoising from its paired original frame as starting point&lt;/p&gt;

&lt;p&gt;Due to the small amount of frames (32 in this case), the whole process is within hours, for one single video, but the output quality still needs to be improved&lt;/p&gt;

&lt;h2 id=&quot;further-details&quot;&gt;Further details&lt;/h2&gt;

&lt;p&gt;The original video is from youtube ‘https://www.youtube.com/channel/UCBcVQr-07MH-p9e2kRTdB3A’, author J Utah, cropped 10 seconds (from 1.5 hour) and to 512x512&lt;/p&gt;

&lt;p&gt;The text prompt is “a abstract painting of a cyberpunk city night, tron robotic, trending on artstation”&lt;/p&gt;

&lt;p&gt;Strength parameter for clockwise: original, 0.45, 0.45, 0.75 (after finetuning, you can lower the strength parameter to get more fidelity, I use 0.45 for comparison, for human 0.325 is good enough)&lt;/p&gt;

&lt;p&gt;Finetuned for 1000 iters (for human only need around 400 iters), 1e-5 lr, late steps 500&lt;/p&gt;

&lt;p&gt;Generation using 50 steps (lazy, nah)&lt;/p&gt;

&lt;p&gt;Using blender for linux to combine the image to videos&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{lei2020dvp,
  title={Blind Video Temporal Consistency via Deep Video Prior},
  author={Lei, Chenyang and Xing, Yazhou and Chen, Qifeng},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{DVP_lei,
  author    = {Chenyang Lei and
               Yazhou Xing and
               Hao Ouyang and
               Qifeng Chen},
  title     = {Deep Video Prior for Video Consistency and Propagation},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {To Appear}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Improve Short Video Consistency With Stable Diffusion</summary></entry><entry><title type="html">Finetune stable diffusion under 24gb vram in hours</title><link href="http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html" rel="alternate" type="text/html" title="Finetune stable diffusion under 24gb vram in hours" /><published>2022-09-12T13:15:31+08:00</published><updated>2022-09-12T13:15:31+08:00</updated><id>http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html">&lt;h1 id=&quot;finetune-stable-diffusion-under-24gb-vram-in-hours&quot;&gt;Finetune Stable Diffusion Under 24GB VRAM In Hours&lt;/h1&gt;

&lt;p&gt;Compared to textual inversion stable diffusion (which needs 10GB+), resume training the original model itself needs more resources, but I have managed to do it using one single RTX 3090Ti, in hours&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tomandjerry_finetune.jpg&quot; alt=&quot;tomandjerry_finetune.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-not-textual-inversion&quot;&gt;Why not textual inversion&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Assume stable diffusion has capabilities of generating all distributions, then textual inversion is the same with resume training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.01618&quot;&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And due to its strong capabilities, everything you wish to finetune on could be expressed as one embedding, for further explanations, please see the original paper&lt;/p&gt;

&lt;p&gt;For most of the case, it works perfectly&lt;/p&gt;

&lt;p&gt;But stable diffusion model does not work on outlier distributions it has never seen, for example, my mom&lt;/p&gt;

&lt;p&gt;No matter how close the embedding leads to the original image, it is not my mom, a lady of the same age and similar head shape and expression is not good enough&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The embedding to distribution loss is too high on this case, can not be ignored, similar cases include highly detailed anime hands and arms which stable diffusion have difficulties in the first place&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On this case, we are going further to get my mom being recognized by stable diffusion, not as a embedding, but a new distribution&lt;/p&gt;

&lt;p&gt;However, let’s respect my mom’s privacy and use Tom and Jerry screenshots as a example instead&lt;/p&gt;

&lt;h2 id=&quot;pre-encode-the-clip-and-f8-embedding-to-free-more-vrams&quot;&gt;Pre-encode the CLIP and f8 embedding to free more vrams&lt;/h2&gt;

&lt;p&gt;The original training/inference config encode text/image pair on the fly, which loads CLIP model into vram, we can not afford it&lt;/p&gt;

&lt;p&gt;And if you are really tight in vram, you can remove the first stage model as well, but totally not recommended, because logging images regularly is important for spoting bugs early&lt;/p&gt;

&lt;p&gt;pre-encode f8:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;posterior = first_stage_model.encode(img_tensor)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;pre-encode CLIP:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;txt_embed = cond_stage_model.encode(text)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And a example config with the pre-encodings instead of CLIP model&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model:
  base_learning_rate: 6.666e-08
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.00085
    cond_stage_key: t5 #actually CLIP for stable-diffusion, pre-encoded, lazy not changing this
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: f8
    conditioning_key: crossattn
    image_size: 64
    channels: 4
    monitor: val/loss_simple_ema
    scale_factor: 0.18215
    use_ema: False

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        # note: this isn\t actually the resolution but
        # the downsampling factor, i.e. this corresnponds to
        # attention on spatial resolution 8,16,32, as the
        # spatial reolution of the latents is 64 for f4
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        num_heads: 8
        use_spatial_transformer: true
        transformer_depth: 1
        use_checkpoint: True
        context_dim: 768
        legacy: False
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        #ckpt_path: /workdir/latent-diffusion/models/first_stage_models/checkpoints/last.ckpt
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.DummyEncoder
      params:
          key: t5 # CLIP, lazy not changing this, works the same
data:
  target: main.WebDataNpyModuleFromConfig
  params:
    batch_size: 1
    num_workers: 8
    training_urls: /workdir/datasets/windows_storage/tomandjerry_finetune.tar
    val_urls: /workdir/datasets/windows_storage/tomandjerry_val.tar
    test_urls: /workdir/datasets/windows_storage/tomandjerry_val.tar
    #null_cond_dropout: 0.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The t5 in config file is actually CLIP, did not change it after first experiment, they are all pre-encodings, so just a lazy typo&lt;/p&gt;

&lt;h2 id=&quot;hack-the-pretrained-stable-diffusion-weights-to-training-checkpoint&quot;&gt;Hack the pretrained stable-diffusion weights to training checkpoint&lt;/h2&gt;

&lt;p&gt;If we are to resume training, we need a training checkpoint to resume on, but the released checkpoint are not for training, so we need to hack it&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python main.py --no-test --base configs/latent-diffusion/finetune_stable_diffusion.yaml -t --gpus 0,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will start a new training from scrach, and we only need the training checkpoint, so we abort training when finishing the first epoch (to spot bugs early, does not need the epoch to finish, abort when you please)&lt;/p&gt;

&lt;p&gt;The checkpoint will be located in logs folder&lt;/p&gt;

&lt;p&gt;Now let’s hack the weights&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_train_dict = torch.load(&quot;/workdir/dev/latent-diffusion.dev/logs/2022-09-06T05-24-48_finetune_stable_diffusion/checkpoints/last.ckpt&quot;, map_location=&quot;cpu&quot;)

tmp_dict = model.state_dict()
keys_list = tmp_dict.keys()
for i in keys_list:
    if &quot;cond_stage_model&quot; not in i:
        model_train_dict['state_dict'][i] = tmp_dict[i]
torch.save(model_train_dict, &quot;/tmp/test_merged.ckpt&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you remember, we removed the CLIP model to free more vrams, so we should skip copying the cond_stage_model&lt;/p&gt;

&lt;p&gt;Now put this checkpoint back into the logs folder, we are good to resume training now&lt;/p&gt;

&lt;h2 id=&quot;resume-training-as-finetuning&quot;&gt;Resume training as finetuning&lt;/h2&gt;

&lt;p&gt;Replace your previous checkpoint folder in command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0 python main.py --resume logs/2022-09-06T05-24-48_finetune_stable_diffusion--base configs/latent-diffusion/finetune_stable_diffusion.yaml -t --gpus 0,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;merge-back-to-release-checkpoint-optional&quot;&gt;Merge back to release checkpoint (optional)&lt;/h2&gt;

&lt;p&gt;After training, you should get a 10GB checkpoint, it would be better if we merge it to the original 4GB checkpoint so that everything is faster&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for k in list(model_train_dict.keys()):
    if k != &quot;state_dict&quot;:
        model_train_dict.pop(k, None)
patch_dict = model.state_dict()
new_patch_dict = {}
for i in patch_dict.keys():
    if i not in model_train_dict['state_dict'].keys():
        model_train_dict['state_dict'][i] = patch_dict[i]
torch.save(model_train_dict, &quot;/tmp/test_merge.ckpt&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you get a 4GB checkpoint, well done&lt;/p&gt;

&lt;h2 id=&quot;limits-and-weak-points&quot;&gt;Limits and weak points&lt;/h2&gt;

&lt;p&gt;During the finetuning process, the stable diffusion model starts to forget other objects in the same catagory, and everything will be biased to match the new finetuning dataset&lt;/p&gt;

&lt;p&gt;After finetuning on Tom And Jerry images, the model starts to draw cat &amp;amp; kittens as tom, and cartoon bears as jerry, even without any prompt related to Tom And Jerry&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tom_biased.jpg&quot; alt=&quot;tom_biased.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;possible-improvements-in-the-future&quot;&gt;Possible improvements in the future&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.12242&quot;&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In their research, a class-specific prior preservation loss is suggested to improve the weak points mentioned above, however, their approach requires more vram to host a original model to gather the loss&lt;/p&gt;

&lt;p&gt;I don’t have the resource to do that, if you got more vram to spare, do try the class-specific prior preservation loss as optional improvement&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{gal2022textual,
      doi = {10.48550/ARXIV.2208.01618},
      url = {https://arxiv.org/abs/2208.01618},
      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
      publisher = {arXiv},
      year = {2022},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@article{ruiz2022dreambooth,
  title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={arXiv preprint arxiv:2208.12242},
  year={2022}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Finetune Stable Diffusion Under 24GB VRAM In Hours</summary></entry><entry><title type="html">Artifact removal of text to image models using diffusion late steps</title><link href="http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html" rel="alternate" type="text/html" title="Artifact removal of text to image models using diffusion late steps" /><published>2022-08-14T15:01:34+08:00</published><updated>2022-08-14T15:01:34+08:00</updated><id>http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html">&lt;h1 id=&quot;artifact-removal-of-text-to-image-models-using-diffusion-late-steps&quot;&gt;Artifact Removal Of Text To Image Models Using Diffusion Late Steps&lt;/h1&gt;

&lt;p&gt;Let’s meet some example artifact images generated on craiyon(formerly dalle-mega, mega version of dalle-mini)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/craiyon_130031_a_girl_standing_in_front_of_a_car.png&quot; alt=&quot;craiyon_130031_a_girl_standing_in_front_of_a_car.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To be honest, these does not look good&lt;/p&gt;

&lt;p&gt;And what we are doing in this blog post, is to make these images look better, effect as follows&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/artifact-removal-test02.png&quot; alt=&quot;artifact-removal-test02.png&quot; /&gt;
&lt;img src=&quot;/assets/artifact-removal-test01.png&quot; alt=&quot;artifact-removal-test01.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;not-a-super-resolution-upscale-problem&quot;&gt;Not a super resolution upscale problem&lt;/h2&gt;

&lt;p&gt;Many people have been using super resolution models such as ESRGAN/SwinIR to upscale their generated image to higher resolution&lt;/p&gt;

&lt;p&gt;But it does not work good on these artifact images, which actually, output a more clear and high resolution nightmare image&lt;/p&gt;

&lt;p&gt;A example with ESRGAN to compare with:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/craiyon_130031_a_girl_standing_in_front_of_a_car_esrgan.jpg&quot; alt=&quot;craiyon_130031_a_girl_standing_in_front_of_a_car_esrgan.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to the nature of image generation models, the composition itself can went all wrong in addition to wrong texture details, and depending on the model ability, the artifact area could vary from pixels to regions, in the example above, the whole face area is terrible, and obviously the whole head wasn’t at a reasonable shape in the first place&lt;/p&gt;

&lt;p&gt;Super resolution does not refine the shape of the head, it does not do re-composition because current degradation method when training the SR model don’t degrade composition&lt;/p&gt;

&lt;h2 id=&quot;not-a-standard-image-to-image-translation-problem&quot;&gt;Not a standard image to image translation problem&lt;/h2&gt;

&lt;p&gt;The super resolution problem is one kind of image to image translation problem, from a low resolution image, to a high resolution image, not working for bad composition images&lt;/p&gt;

&lt;p&gt;However, what if we try to deal with the artifact images as a img2img problem, &lt;strong&gt;from a bad image, to a good image&lt;/strong&gt;, sounds good ?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;People have tried, and I do too&lt;/strong&gt;, I tried to use vqgan-f8 as a degradation method that ends up wrong composition images, then refine it back to a vqgan-f4 image, expecting a better face for generated human figure&lt;/p&gt;

&lt;p&gt;When training, results seems good, very good, images are remarkably more realistic and details are corrected, very obvious&lt;/p&gt;

&lt;p&gt;When validating, results seems remain good enough, not as good as training set for certain, but most people can tell the image quality has improvement&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When testing against real artifact images, nobody can tell which is which, even the ‘refined’ image is worse&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So here I discovered the following assumptions&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Artifact images contains compositional error, and it is mainly introduced by model learning not vqgan degradation&lt;/li&gt;
  &lt;li&gt;There are well generated images shown remarkable quality using the same model generates artifact images, what makes it interesting is that these well generated images has significant lower vqgan-f8 degradation as well, proving there to be heavy bias during model training makes some images are a lot more concerned, and vice versa&lt;/li&gt;
  &lt;li&gt;Classifier free guidance in latent-diffusion laion-400m model can generate more better quality images&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;So the artifact images are actually the outliers in the training dataset, without proper learning at both vqgan/composition stage, but close enough with the text prompt to be chosen&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So this is not a simple image to image problem, we may regard this as ‘how to improve the original model to do better at outlier cases’&lt;/p&gt;

&lt;h2 id=&quot;diffusion-late-steps&quot;&gt;Diffusion late steps&lt;/h2&gt;

&lt;p&gt;Inspired by SDEdit paper, we may remove artifact of any kind by a reverse stochastic process using a diffusion model&lt;/p&gt;

&lt;p&gt;The most fascinating part of this is we do not need a full diffusion model to refine the artifact image, if with luck, we only need several late steps be trained and skip the rest, this would save lots of computing power for a refinement task&lt;/p&gt;

&lt;p&gt;And to further reduce the training cost, I used latent-diffusion with vqgan-f4, combined with lesser steps in the diffusion model design, it’s quite possible to finish it under low computing restrictions&lt;/p&gt;

&lt;p&gt;Now the question is: how many late steps is enough ?&lt;/p&gt;

&lt;p&gt;That depends on the artifact severeness, for a default 1000 steps in total, I would recommend training at 500-750 steps and skip 1-500/751-1000 at first, then finetune the 751-1000 steps training using the 500-750 steps model&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I actually trained at 751-1000 then finetuned to 500-750, because I found 250 late steps is not enough&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;And of course, you can train 500-1000 all together, if the model has enough parameters and you got enough compute&lt;/p&gt;

&lt;p&gt;Example limiting late steps when training:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()
t = torch.randint(250, 500, (x.shape[0],), device=self.device).long()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example using late steps to refine when inferencing:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;t = repeat(torch.tensor([500]), '1 -&amp;gt; b', b=1)
t = t.to(&quot;cuda&quot;).long()
noise = torch.randn_like(x_T)
x_T = model.q_sample(x_start=x_T, t=t, noise=noise)

#put x_T into ddim and hardcode the last steps
for i, step in enumerate(iterator):
    if last_steps:
        if i &amp;lt; (S - last_steps):
            continue
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The 500-750 steps could refine the shape of the head (middle image), then 750-1000 steps refine the details (right image)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/late_diffusion_compare.png&quot; alt=&quot;late_diffusion_compare.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;latent diffusion models trained using &lt;a href=&quot;https://github.com/CompVis/latent-diffusion&quot;&gt;https://github.com/CompVis/latent-diffusion&lt;/a&gt; , modifications on limiting diffusion late steps&lt;/p&gt;

&lt;p&gt;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations, inspired by the process of reverse stochastic process in paper, not using the code, and the training method in this blog is original not from this paper&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2108.01073,
  doi = {10.48550/ARXIV.2108.01073},
  
  url = {https://arxiv.org/abs/2108.01073},
  
  author = {Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Artifact Removal Of Text To Image Models Using Diffusion Late Steps</summary></entry><entry><title type="html">A Closer Look Into The latent-diffusion Repo, Do Better Than Just Looking</title><link href="http://localhost:4000/jekyll/update/2022/05/16/A-closer-look-into-the-latent-diffusion-repo-do-better-than-just-looking.html" rel="alternate" type="text/html" title="A Closer Look Into The latent-diffusion Repo, Do Better Than Just Looking" /><published>2022-05-16T14:16:02+08:00</published><updated>2022-05-16T14:16:02+08:00</updated><id>http://localhost:4000/jekyll/update/2022/05/16/A-closer-look-into-the-latent-diffusion-repo-do-better-than-just-looking</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/05/16/A-closer-look-into-the-latent-diffusion-repo-do-better-than-just-looking.html">&lt;h1 id=&quot;a-closer-look-into-the-latent-diffusion-repo-do-better-than-just-looking&quot;&gt;A Closer Look Into The latent-diffusion Repo, Do Better Than Just Looking&lt;/h1&gt;

&lt;p&gt;The two stage compress-then-diffusion boosts training efficiency dramatically, which made low computing art creations possible&lt;/p&gt;

&lt;p&gt;For those readers who aren’t familiar with &lt;a href=&quot;https://github.com/CompVis/latent-diffusion&quot;&gt;https://github.com/CompVis/latent-diffusion&lt;/a&gt; , please see &lt;a href=&quot;https://arxiv.org/abs/2112.10752&quot;&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I would assume that you already tried or is going to try out this model-combo for your own artistic datasets&lt;/p&gt;

&lt;p&gt;The authors already released some pretrained vq/kl regularized autoencoder models, &lt;strong&gt;If your dataset looks like openimages dataset, or mathematically speaking your dataset have a similar visual feature distribution with the openimages dataset, then you’re in luck, just grab one pretrained autoencoder and train your ldm with demo configs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;But what if your own dataset is visually very not similar from openimages, for example, danbooru anime dataset ?&lt;/p&gt;

&lt;h2 id=&quot;the-pretrained-autoencoder-is-not-a-silver-bullet&quot;&gt;The pretrained autoencoder is not a silver bullet&lt;/h2&gt;

&lt;p&gt;Let’s give the pretrained vq-f4 a reconstruction shot at danbooru images&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/151115.jpg&quot; alt=&quot;151115&quot; /&gt;
&lt;img src=&quot;/assets/151115_rec.jpg&quot; alt=&quot;151115_rec&quot; /&gt;
&lt;img src=&quot;/assets/80100.jpg&quot; alt=&quot;80100&quot; /&gt;
&lt;img src=&quot;/assets/80100_rec.jpg&quot; alt=&quot;80100_rec&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hmmm, the eyes are a little off, but it looks kinda fine&lt;/p&gt;

&lt;p&gt;Then the pretrained vq-f8:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/151115.jpg&quot; alt=&quot;151115&quot; /&gt;
&lt;img src=&quot;/assets/151115_rec_f8.jpg&quot; alt=&quot;151115_rec_f8&quot; /&gt;
&lt;img src=&quot;/assets/80100.jpg&quot; alt=&quot;80100&quot; /&gt;
&lt;img src=&quot;/assets/80100_rec_f8.jpg&quot; alt=&quot;80100_rec_f8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oh no, this is giving me nightmare&lt;/p&gt;

&lt;p&gt;So there is no need to test out the vq-f16, vq-f8 is compressing way too much&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The question is: is the pretrained vq-f4 on openimages good enough for danbooru dataset ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Well, considering I already found that re-train a vq-f4 only takes one or two epochs, it’s really not necessery to endure with the eye detail gliches, as well as the biased codebook distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;But even if the autoencoder training takes long, I still wouldn’t chose to use the pretrained vq-f4 on danbooru dataset, not only because the ‘best reconstruction’ is not good enough, the distribution of the codebook entries are very different than the danbooru dataset as well, it means that somewhere between a dress fiber texture code and hair strand texture code, there is a squirrel fur texture code should not be used, but will be optimized during later diffusion training, I have no idea what consequence it shall make but definitely not favoring it&lt;/p&gt;

&lt;h2 id=&quot;selecting-a-proper-autoencoder-config&quot;&gt;Selecting a proper autoencoder config&lt;/h2&gt;

&lt;p&gt;I have tried vq-f16/vq-f8/vq-f4 with the default config and the trainable parameter count are near identical due to gpu limit&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generally speaking, if you are creating 256x256 images, vq-f4 leads to a 64x64 diffusion model training, after my experiments, this is the best combo config that works really well&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;And after trying vq-f8, I found it hard to sustain the reconstrution quality without ramping up model abilities, with the same trainable parameters, it does’t do details well, another problem is if 64x64 diffusion model is training as fast as in 5 days on my case, spend more time on autoencoders in exchange for a easier diffusion training does not seem to be worth it&lt;/p&gt;

&lt;p&gt;I also tried vq-f16 on my first attempt, nah… , it’s working, but after two weeks time, it doesn’t seem to be more impressive, all the gloomy details drives me mad, compared with vq-f4 + 64x64 diffusion, it’s totally not worth it, unless you wanna try 1024x1024 high resolution image generation, which makes vq-f16 + 64x64 diffusion seems proper, but that shall be a different story then&lt;/p&gt;

&lt;h2 id=&quot;conditioning-on-keypoints&quot;&gt;Conditioning on keypoints&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/CompVis/latent-diffusion&quot;&gt;https://github.com/CompVis/latent-diffusion&lt;/a&gt; already give examples about how to use cross attention to do conditional training, such as conditioning on image/class_label/object_center_point/depth_map etc, and I just made the 17 keypoints into a [1, 17] tensor, just as the object center point hack&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def tokenize_coordinates(self, x: float, y: float, dim:int) -&amp;gt; int:
        &quot;&quot;&quot;
        Express 2d coordinates with one number.
        Example: assume self.no_tokens = 16, then no_sections = 4:
        0  0  0  0
        0  0  #  0
        0  0  0  0
        0  0  0  x
        Then the # position corresponds to token 6, the x position to token 15.
        @param x: float in [0, 1]
        @param y: float in [0, 1]
        @return: discrete tokenized coordinate
        &quot;&quot;&quot;
        #x_discrete = int(round(x * (self.512 - 1)))
        #y_discrete = int(round(y * (self.512 - 1)))
        if x &amp;gt; (dim - 1):
            x = (dim - 1)
        if y &amp;gt; (dim - 1):
            y = (dim - 1)
        if x &amp;lt; 0:
            x = 0
        if y &amp;lt; 0:
            y = 0
        return int(round(x)) + int(round(y) * dim)

    def coordinates_from_token(self, token: int, dim: int) -&amp;gt; (float, float):
        x = token % dim
        y = token // dim
        return x, y

    def build_tensor_from_kps(self, kps, dim):
        kps_names = ['nose',
                     'eye_left',
                     'eye_right',
                     'ear_left',
                     'ear_right',
                     'shoulder_left',
                     'shoulder_right',
                     'elbow_left',
                     'elbow_right',
                     'wrist_left',
                     'wrist_right',
                     'hip_left',
                     'hip_right',
                     'knee_left',
                     'knee_right',
                     'ankle_left',
                     'ankle_right']
        tokens = []
        for name in kps_names:
           x = kps[name][0]
           y = kps[name][1]
           if dim != 512:
               x = x // (512/dim)
               y = y // (512/dim)
           _token = self.tokenize_coordinates(x, y, dim)
           tokens.append(_token)
        #return LongTensor(tokens)
        return Tensor(tokens)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;A custom dataloader to load the json keypoints file&lt;/li&gt;
  &lt;li&gt;A DummyEncoder to construct the [1, 17] tensor from keypoints&lt;/li&gt;
  &lt;li&gt;Add keypoints option to the conditioning key for the model, modify log_images function to log conditioning keypoints as well&lt;/li&gt;
  &lt;li&gt;Adapt the config to use keypoints conditioning and enable spartial transformer cross attention options&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The complete code can be found at &lt;a href=&quot;https://github.com/lxj616/latent-diffusion&quot;&gt;https://github.com/lxj616/latent-diffusion&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;training-the-diffusion-model&quot;&gt;Training the diffusion model&lt;/h2&gt;

&lt;p&gt;I decreased the model_channels to 160 (from 256) to save vram for larger batches, everything else is more or less copied from example config&lt;/p&gt;

&lt;p&gt;This config file can be found at configs/latent-diffusion/danbooru-keypoints-ldm-vq-4.yaml in my latent-diffusion fork repo&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model:
  base_learning_rate: 2.0e-06
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    cond_stage_key: keypoints
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    conditioning_key: crossattn
    image_size: 64
    channels: 3
    monitor: val/loss_simple_ema
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 64
        in_channels: 3
        out_channels: 3
        model_channels: 160
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 4
        num_heads: 8
        use_spatial_transformer: true
        transformer_depth: 1
        context_dim: 17
        legacy: false
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 3
        n_embed: 8192
        ckpt_path: /workdir/taming-transformers/logs/2022-04-25T12-37-06_custom_vqgan/checkpoints/last.ckpt
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.DummyEncoder
      params:
        key: keypoints
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 8
    wrap: false
    train:
      target: ldm.data.custom_616.CustomTrain
      params:
        training_images_list_file: /workdir/datasets/danbooru_tiny.list
        size: 256
    validation:
      target: ldm.data.custom_616.CustomTest
      params:
        test_images_list_file: /workdir/datasets/danbooru_tiny_test.list
        size: 256

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After 20 epochs of training, I suspect there is some overfitting, and keep observing until epoch 33, it became more and more obvious&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/progressive_row_gs-380000_e-000033_b-004988_cropped.jpg&quot; alt=&quot;overfitting&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model is trying to denoise some tiny details in the last steps, only to make the details worse, this is very dataset specific, too much noise in the danbooru dataset make details impossible to refine, as well as lack of data in total&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So by calculation 20 epochs is good enough, which is 2022-04-30 ~ 2022-05-04 (5 days), the vq-f4 training took a day, so this is within a week on a single 3090 Ti&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Maybe I should not have filtered the dataset too heavy losing too much slightly noisy data …&lt;/p&gt;

&lt;h2 id=&quot;sample-using-pose-keypoints&quot;&gt;Sample using pose keypoints&lt;/h2&gt;

&lt;p&gt;In short, parse the conditioning to ddim sampler, and you’ll get the conditioned output&lt;/p&gt;

&lt;p&gt;There is a demo script at scripts/sample_pose_diffusion.py in my latent-diffusion fork repo&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;samples, intermediates = ddim.sample(steps, conditioning=cond, batch_size=bs, shape=shape, eta=eta, verbose=False,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And there are many hidden tricks that ddim already have, such as inpainting, priming on a different noise latent …&lt;/p&gt;

&lt;p&gt;When I tried to illustrate the final output quality, &lt;strong&gt;I chose a real world pose example from a short video&lt;/strong&gt;, instead of using poses from the training/validation set, this is more fun and fair to demonstrate the model capabilities&lt;/p&gt;

&lt;p&gt;cherry-picked:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/000068_1.jpg&quot; alt=&quot;vq-f4 ddpm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;random sample:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/total_grid.jpg&quot; alt=&quot;random sample&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However if you really need to generate better quality images, you can also consider &lt;strong&gt;the “truncation trick”&lt;/strong&gt; similar to stylegan/biggan, but in our case is to select &lt;strong&gt;“the most common top N poses”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ha, gotcha, there is no common pose in the dataset, the keypoints are way too scattered to be in common, there are total 17 points everywhere, how could they possible accidentally be the same ? The ‘pose available space’ is 87112285931760246646623899502532662132736 large(I regularized the coordinate to be within 16x16 grids each, math.pow(256, 17)), good luck finding a most common pose to get the trick done&lt;/p&gt;

&lt;p&gt;The following pose is from the top row image in the training set, the generated output image is with similar pose but not identical with the training set image, there must be a similar image in the dataset though, if you don’t mind, selecting poses from the training set can give you better results, but not as fun as selecting real world poses to challenge the model, and to further improve the output quality by cheating, select a top common pose rather than random training set pose&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ldm_example.jpg&quot; alt=&quot;VQGAN f4 With latent diffusion&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;failure-attempts&quot;&gt;Failure attempts&lt;/h2&gt;

&lt;p&gt;I tried to further clean up the danbooru dataset subset, reducing the noisy images and try vq-f8 to get details right and blazing fast, ended up worse output quality due to lack of data, details see my last post about datasets&lt;/p&gt;

&lt;p&gt;I tried to clean up anime image backgrounds, it wasn’t accurate enough, introduces new random image feature noises, not working&lt;/p&gt;

&lt;p&gt;I forgot to set spartial transformer in the config, find that out after many days when the log image can be clearly distinguished, my heart is broken, especially when I see the code comment after carefully debug&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        if use_spatial_transformer:
            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'

        if context_dim is not None:
            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'
            from omegaconf.listconfig import ListConfig
            if type(context_dim) == ListConfig:
                context_dim = list(context_dim)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;latent diffusion models trained using &lt;a href=&quot;https://github.com/CompVis/latent-diffusion&quot;&gt;https://github.com/CompVis/latent-diffusion&lt;/a&gt; , modifications on keypoints conditioning&lt;/p&gt;

&lt;p&gt;vq regularized models trained using &lt;a href=&quot;https://github.com/CompVis/taming-transformers&quot;&gt;https://github.com/CompVis/taming-transformers&lt;/a&gt; , no modifications&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{esser2020taming,
      title={Taming Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Robin Rombach and Björn Ommer},
      year={2020},
      eprint={2012.09841},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">A Closer Look Into The latent-diffusion Repo, Do Better Than Just Looking</summary></entry><entry><title type="html">Rethinking The Danbooru 2021 Dataset</title><link href="http://localhost:4000/jekyll/update/2022/05/15/rethinking-the-danbooru-2021-dataset.html" rel="alternate" type="text/html" title="Rethinking The Danbooru 2021 Dataset" /><published>2022-05-15T01:11:18+08:00</published><updated>2022-05-15T01:11:18+08:00</updated><id>http://localhost:4000/jekyll/update/2022/05/15/rethinking-the-danbooru-2021-dataset</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2022/05/15/rethinking-the-danbooru-2021-dataset.html">&lt;h1 id=&quot;rethinking-the-danbooru-2021-dataset&quot;&gt;Rethinking The Danbooru 2021 Dataset&lt;/h1&gt;

&lt;p&gt;I trained a keypoint based anime generation model on top of the danbooru 2021 dataset, more specifically, on a filtered subset, and get some satisfying results&lt;/p&gt;

&lt;p&gt;But after everything is done, the whole process need to be reviewed, I need to do backpropagation towards my mind and do better next time&lt;/p&gt;

&lt;p&gt;So here comes the question: &lt;strong&gt;which problems are dataset related and how do they affect the later training process&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;addressing-the-known-problems-discussed-in-years&quot;&gt;Addressing the known problems discussed in years&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Hands have long been a weak point (https://www.gwern.net/Faces)&lt;/li&gt;
  &lt;li&gt;The danbooru dataset is way too noisy (reddit user comments)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To train a pose keypoints based model, a pose keypoints dataset is required, but not all danbooru dataset images is suitable for training&lt;/p&gt;

&lt;h2 id=&quot;my-approach-to-aquire-a-cleaner-subset&quot;&gt;My approach to aquire a cleaner subset&lt;/h2&gt;

&lt;p&gt;Let’s take a look at https://www.gwern.net/Danbooru2021 offical grid sample image&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/danbooru2020-512px-samples.jpg&quot; alt=&quot;danbooru2020-512px-samples.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please be noted that this is from a SFW subset (around 3m+), and down-scaled to 512x512 already&lt;/p&gt;

&lt;p&gt;For the scenario of “keypoints based” anime generation, it’s easy to tell most of the samples are &lt;strong&gt;not suitable for training&lt;/strong&gt;, naming a few:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;book grid line sketch manga&lt;/li&gt;
  &lt;li&gt;multiple people&lt;/li&gt;
  &lt;li&gt;a girl making weird poses that the feet is too big and no arms&lt;/li&gt;
  &lt;li&gt;back facing the camera&lt;/li&gt;
  &lt;li&gt;a landscape photo&lt;/li&gt;
  &lt;li&gt;a calender cover&lt;/li&gt;
  &lt;li&gt;the girl is holding a doll face, and all backgroud full of doll face&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Among the 10x10=100 samples, &lt;strong&gt;basic counting tells that &amp;lt; 20 samples meet the basic requirement&lt;/strong&gt; “is a portrait with pose keypoints”&lt;/p&gt;

&lt;p&gt;So here we expect making a &lt;strong&gt;600k(20% of 3m)&lt;/strong&gt; subset and they may still not be suitable for training&lt;/p&gt;

&lt;p&gt;Before I utilized CLIP text based filtering to clean the dataset, I found that 3m+ images is way too large for a deep learning model sweep (later I realized this is a misjudge)&lt;/p&gt;

&lt;p&gt;And after labeling every unwanted sample image CLIP score, I choose a threshold (with human examine sampling) of 600k to be the intermediate subset of the description “is a portrait with pose keypoints”&lt;/p&gt;

&lt;p&gt;Next I labeled all the 600k image samples with https://github.com/ShuhongChen/bizarre-pose-estimator , getting pose keypoints&lt;/p&gt;

&lt;p&gt;Now it’s time for some basic data analysis to cluster the poses&lt;/p&gt;

&lt;p&gt;As a example, here is plotting the “middle of the two hip keypoint” with dbscan clustering&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/Figure_1.png&quot; alt=&quot;Figure_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Turns out the dbscan clustering is totally unnecesary, just simply plot it and the answer is obvious&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;sometimes hip is out of the image scope, such as half portrait may only have top upper body, so y &amp;gt;= 512 is totally understandable&lt;/li&gt;
  &lt;li&gt;when something went wrong with the image or pose-estimation model, random points are understandable, such as some weird four legged creature may have hip anywhere&lt;/li&gt;
  &lt;li&gt;the dense area of the main distribution seems to be normal, regarding one single ‘hip position’ alone, are they good samples for training ?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Wait, I have a fun quiz about the fore-mentioned figure:&lt;/p&gt;

&lt;p&gt;Under what circumstances should a anime &lt;strong&gt;have hips top of the image like y &amp;lt; 100&lt;/strong&gt; ?&lt;/p&gt;

&lt;p&gt;Ans:&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;Show the case&lt;/summary&gt;
  &lt;p&gt; 
  &lt;img src=&quot;/assets/2106.jpg&quot; /&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;p&gt;XD&lt;/p&gt;

&lt;p&gt;Finally, applying several data analysis techniques, &lt;strong&gt;I finally got a 363k subset&lt;/strong&gt; which is ~50% smaller than the previous intermediate 600k subset, make sure every shoulder and wrist etc etc not placing too odd&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Maybe this filtering is a little bit overhead, sometimes I felt like this type of filtering does not eliminate most abnormal samples but hurt total available image count directly&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;rethink-a-cleaner-subset-is-not-clean-enough&quot;&gt;Rethink: A cleaner subset is not clean enough&lt;/h2&gt;

&lt;p&gt;Here’s 20 random samples from the 363k subset&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/20_concat.jpg&quot; alt=&quot;20_concat.jpg&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;top row 8/20 (40%) images seems to be near-unified portraits &lt;strong&gt;suitable for training&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;mid row 6/20 (30%) images seems to be &lt;strong&gt;questionable&lt;/strong&gt;, not sure if the model could refine details from such stylized complex-visual image&lt;/li&gt;
  &lt;li&gt;bottom row 6/20 (30%) images is totally &lt;strong&gt;unacceptable&lt;/strong&gt;, it shall make the training unstable and semantically confused&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now to recap the problems we mention earlier&lt;/p&gt;

&lt;h3 id=&quot;hands-have-long-been-a-weak-point&quot;&gt;Hands have long been a weak point&lt;/h3&gt;

&lt;p&gt;If your dataset only ~40% contains standard looking hands, and ~60% images the hand is holding some item or does not have hands at all, your model are not going to generate hands well&lt;/p&gt;

&lt;p&gt;By intuition the next step is to further clean up the dataset, selecting only the appropriate 40% (top row as example), make it 140k in total and finally getting better results&lt;/p&gt;

&lt;p&gt;Well, I tried, making a 101k subset out of the 364k subset, but I can not get it ‘selecting only the appropriate 40%’, by statistics they look alike, the best way I can come up with is to train another resnet model to label them, but this dataset is different from the leonid afremov dataset, I can hand craft segmentation 25% of the 600 paintings, but there is no way I tag sufficient percentage of this 363k dataset all by myself&lt;/p&gt;

&lt;p&gt;I finally made a 101k subset towards ‘the most usual poses’ by statistics, and it does not do well, too less data regarding too much poses&lt;/p&gt;

&lt;h3 id=&quot;the-danbooru-dataset-is-way-too-noisy&quot;&gt;The danbooru dataset is way too noisy&lt;/h3&gt;

&lt;p&gt;Even with all the efforts to clean the dataset, in the final sampling stage, it is easy to spot totally undesirable outputs such as below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bad_sample_000032.jpg&quot; alt=&quot;bad_sample_000032.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There must be a cool white hair knight wearing leather armor so cool so dark in the dataset, and totally not like any of the anime cute girls wearing dresses&lt;/p&gt;

&lt;p&gt;However, the pose is correct, at least, a cool white hair leather armor knight is still anime, I guess&lt;/p&gt;

&lt;p&gt;In a different perspective, it also could meant that there isn’t enough similar images in the dataset, a dozen more leather armor knight images should allow the model to draw better&lt;/p&gt;

&lt;p&gt;A more promising approach to deal with noisy dataset is to ramp up the model ability like TADNE “model size increase (more than doubles, to 1GB total)”, aydao did a good job on other hacks as well, but in my situation I chose to try the opposite, to make training time as low as 4~9 days with one single gpu thus can not afford to double the model size, at all, and as a consequence, I filtered out 90% of the dataset images instead of training on the full/intermediate dataset&lt;/p&gt;

&lt;h2 id=&quot;unfinished-exploration&quot;&gt;Unfinished exploration&lt;/h2&gt;

&lt;p&gt;If I were to do it again, with the lessions learnt in a hard way, I would carefully carry it out in the following order:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CLIP filtering at the very first place, towards full 3m+ dataset, don’t do image feature clustering (didn’t mention in this article), just CLIP out the majority unwanted images, leave the rest for later procedures&lt;/li&gt;
  &lt;li&gt;assume the first stage already filtered out more than half of the images, tag the rest with pose estimator https://github.com/ShuhongChen/bizarre-pose-estimator, filter them soft and gentle, don’t go too far&lt;/li&gt;
  &lt;li&gt;if manual sampling from the subset observes obvious type of bad case, a lot, and assume CLIP doesn’t help in this particular case, do some coding to deal with it, example: too much black borders with too little content&lt;/li&gt;
  &lt;li&gt;manually tag 1% of the dataset, train a reset model, testing 5% of the dataset, correct the prediction and re-train with 5% of the dataset, then testing 25% of the dataset, correct the prediction again then re-train on 25% of the dataset, get the whole dataset filtered (I tried this method to generate a 70k dataset on other experiments, it works really well, but time consuming), I guess this step could take weeks for a dataset as large as danbooru even pre-filtered into intermediate subset&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As for the “image feature clustering”, I already regret doing so, it does not rule out the “white hair knight wearing leather armor” case, It does not deal with “too large black border too little content” case, and easy to spot weird images can be filtered either by CLIP or pose-estimator, the bottleneck is not the GPU speed, I found the reason of my slow inferencing speed is due to the USB hard drive I store the 3m+ images on, BTW, I lost all data on that drive later, one should never use USB hard drive to store massive amount of images&lt;/p&gt;

&lt;p&gt;I assume that if everything went well, there would be a near 150k pose keypoint image subset, around 70k best quality images and 80k sort-of-complex images, and no white hair knight wearing leather armor !&lt;/p&gt;

&lt;p&gt;Or if you got more computing power to spare, filter the dataset more gently, allow a slightly noisier but overall much larger dataset may improve training, my attempt training with a 101k subset(compared to 363k) ends up damaging overall generation quality&lt;/p&gt;

&lt;p&gt;But that will be other warriors’ adventure, I’ll upload the 363k keypoints if anyone is interested, the filename is the image id, you could download the corresponding image from danbooru 2021 https://www.gwern.net/Danbooru2021, follow the webpage instructions and you can only download images with corresponding id in the shard, or download the whole SFW subset then fetch the image locally if wish not to read long instructions&lt;/p&gt;

&lt;p&gt;The json file for each image contains 17 keypoints just like coco dataset, and is the output of https://github.com/ShuhongChen/bizarre-pose-estimator, you can generate your own image keypoints using this repo, one example below&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{&quot;nose&quot;: [176.8, 256.0], &quot;eye_left&quot;: [150.4, 282.40000000000003], &quot;eye_right&quot;: [168.0, 247.20000000000002], &quot;ear_left&quot;: [150.4, 322.00000000000006], &quot;ear_right&quot;: [181.20000000000002, 234.0], &quot;shoulder_left&quot;: [238.4, 374.80000000000007], &quot;shoulder_right&quot;: [264.8, 251.60000000000002], &quot;elbow_left&quot;: [348.40000000000003, 361.6000000000001], &quot;elbow_right&quot;: [361.6000000000001, 251.60000000000002], &quot;wrist_left&quot;: [445.20000000000005, 427.6000000000001], &quot;wrist_right&quot;: [388.00000000000006, 225.20000000000002], &quot;hip_left&quot;: [533.2, 401.20000000000005], &quot;hip_right&quot;: [414.40000000000003, 286.80000000000007], &quot;knee_left&quot;: [352.80000000000007, 220.8], &quot;knee_right&quot;: [405.6000000000001, 150.4], &quot;ankle_left&quot;: [396.80000000000007, 128.4], &quot;ankle_right&quot;: [392.40000000000003, 128.4]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To visualize, use the same way visualizing coco dataset, a example can be found in my forked latent-diffusion condition logging functions, which borrows from bizarre-pose-estimator code repo and is originally from coco dataset utilities&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/conditioning_gs-160000_e-000014_b-000904.jpg&quot; alt=&quot;conditioning keypoints log&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Keypoints tar ball:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/1KqdDfUJQkY-8MoQhnCCTXq-YpDciZlco/view?usp=sharing&quot;&gt;https://drive.google.com/file/d/1KqdDfUJQkY-8MoQhnCCTXq-YpDciZlco/view?usp=sharing&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;danbooru 2021 dataset originally contains 4.9m+ images, here I filtered out 363k subset, then further made a 101k tiny subset for further testing, https://www.gwern.net/Danbooru2021&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{danbooru2021,
    author = {Anonymous and Danbooru community and Gwern Branwen},
    title = {Danbooru2021: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset},
    howpublished = {\url{https://www.gwern.net/Danbooru2021}},
    url = {https://www.gwern.net/Danbooru2021},
    type = {dataset},
    year = {2022},
    month = {January},
    timestamp = {2022-01-21},
    note = {Accessed: DATE} }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2108.01819,
  doi = {10.48550/ARXIV.2108.01819},
  url = {https://arxiv.org/abs/2108.01819},
  author = {Chen, Shuhong and Zwicker, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Transfer Learning for Pose Estimation of Illustrated Characters},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">Rethinking The Danbooru 2021 Dataset</summary></entry></feed>