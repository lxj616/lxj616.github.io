<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Finetune stable diffusion under 24gb vram in hours | Something I found</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Finetune stable diffusion under 24gb vram in hours" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Finetune Stable Diffusion Under 24GB VRAM In Hours" />
<meta property="og:description" content="Finetune Stable Diffusion Under 24GB VRAM In Hours" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html" />
<meta property="og:site_name" content="Something I found" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-12T13:15:31+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Finetune stable diffusion under 24gb vram in hours" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Finetune stable diffusion under 24gb vram in hours","dateModified":"2022-09-12T13:15:31+08:00","datePublished":"2022-09-12T13:15:31+08:00","url":"http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html"},"description":"Finetune Stable Diffusion Under 24GB VRAM In Hours","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Something I found" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Something I found</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Finetune stable diffusion under 24gb vram in hours</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-09-12T13:15:31+08:00" itemprop="datePublished">Sep 12, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="finetune-stable-diffusion-under-24gb-vram-in-hours">Finetune Stable Diffusion Under 24GB VRAM In Hours</h1>

<p>Compared to textual inversion stable diffusion (which needs 10GB+), resume training the original model itself needs more resources, but I have managed to do it using one single RTX 3090Ti, in hours</p>

<p><img src="/assets/tomandjerry_finetune.jpg" alt="tomandjerry_finetune.jpg" /></p>

<h2 id="why-not-textual-inversion">Why not textual inversion</h2>

<p><strong>Assume stable diffusion has capabilities of generating all distributions, then textual inversion is the same with resume training</strong></p>

<p><a href="https://arxiv.org/abs/2208.01618">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></p>

<p>And due to its strong capabilities, everything you wish to finetune on could be expressed as one embedding, for further explanations, please see the original paper</p>

<p>For most of the case, it works perfectly</p>

<p>But stable diffusion model does not work on outlier distributions it has never seen, for example, my mom</p>

<p>No matter how close the embedding leads to the original image, it is not my mom, a lady of the same age and similar head shape and expression is not good enough</p>

<p><strong>The embedding to distribution loss is too high on this case, can not be ignored, similar cases include highly detailed anime hands and arms which stable diffusion have difficulties in the first place</strong></p>

<p>On this case, we are going further to get my mom being recognized by stable diffusion, not as a embedding, but a new distribution</p>

<p>However, let’s respect my mom’s privacy and use Tom and Jerry screenshots as a example instead</p>

<h2 id="pre-encode-the-clip-and-f8-embedding-to-free-more-vrams">Pre-encode the CLIP and f8 embedding to free more vrams</h2>

<p>The original training/inference config encode text/image pair on the fly, which loads CLIP model into vram, we can not afford it</p>

<p>And if you are really tight in vram, you can remove the first stage model as well, but totally not recommended, because logging images regularly is important for spoting bugs early</p>

<p>pre-encode f8:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>posterior = first_stage_model.encode(img_tensor)
</code></pre></div></div>

<p>pre-encode CLIP:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>txt_embed = cond_stage_model.encode(text)
</code></pre></div></div>

<p>And a example config with the pre-encodings instead of CLIP model</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model:
  base_learning_rate: 6.666e-08
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.00085
    cond_stage_key: t5 #actually CLIP for stable-diffusion, pre-encoded, lazy not changing this
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: f8
    conditioning_key: crossattn
    image_size: 64
    channels: 4
    monitor: val/loss_simple_ema
    scale_factor: 0.18215
    use_ema: False

    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions:
        # note: this isn\t actually the resolution but
        # the downsampling factor, i.e. this corresnponds to
        # attention on spatial resolution 8,16,32, as the
        # spatial reolution of the latents is 64 for f4
        - 4
        - 2
        - 1
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 4
        - 4
        num_heads: 8
        use_spatial_transformer: true
        transformer_depth: 1
        use_checkpoint: True
        context_dim: 768
        legacy: False
    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        #ckpt_path: /workdir/latent-diffusion/models/first_stage_models/checkpoints/last.ckpt
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: ldm.modules.encoders.modules.DummyEncoder
      params:
          key: t5 # CLIP, lazy not changing this, works the same
data:
  target: main.WebDataNpyModuleFromConfig
  params:
    batch_size: 1
    num_workers: 8
    training_urls: /workdir/datasets/windows_storage/tomandjerry_finetune.tar
    val_urls: /workdir/datasets/windows_storage/tomandjerry_val.tar
    test_urls: /workdir/datasets/windows_storage/tomandjerry_val.tar
    #null_cond_dropout: 0.2
</code></pre></div></div>

<p>The t5 in config file is actually CLIP, did not change it after first experiment, they are all pre-encodings, so just a lazy typo</p>

<h2 id="hack-the-pretrained-stable-diffusion-weights-to-training-checkpoint">Hack the pretrained stable-diffusion weights to training checkpoint</h2>

<p>If we are to resume training, we need a training checkpoint to resume on, but the released checkpoint are not for training, so we need to hack it</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CUDA_VISIBLE_DEVICES=0 python main.py --no-test --base configs/latent-diffusion/finetune_stable_diffusion.yaml -t --gpus 0,
</code></pre></div></div>

<p>This will start a new training from scrach, and we only need the training checkpoint, so we abort training when finishing the first epoch (to spot bugs early, does not need the epoch to finish, abort when you please)</p>

<p>The checkpoint will be located in logs folder</p>

<p>Now let’s hack the weights</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_train_dict = torch.load("/workdir/dev/latent-diffusion.dev/logs/2022-09-06T05-24-48_finetune_stable_diffusion/checkpoints/last.ckpt", map_location="cpu")

tmp_dict = model.state_dict()
keys_list = tmp_dict.keys()
for i in keys_list:
    if "cond_stage_model" not in i:
        model_train_dict['state_dict'][i] = tmp_dict[i]
torch.save(model_train_dict, "/tmp/test_merged.ckpt")
</code></pre></div></div>

<p>If you remember, we removed the CLIP model to free more vrams, so we should skip copying the cond_stage_model</p>

<p>Now put this checkpoint back into the logs folder, we are good to resume training now</p>

<h2 id="resume-training-as-finetuning">Resume training as finetuning</h2>

<p>Replace your previous checkpoint folder in command</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CUDA_VISIBLE_DEVICES=0 python main.py --resume logs/2022-09-06T05-24-48_finetune_stable_diffusion--base configs/latent-diffusion/finetune_stable_diffusion.yaml -t --gpus 0,
</code></pre></div></div>

<h2 id="merge-back-to-release-checkpoint-optional">Merge back to release checkpoint (optional)</h2>

<p>After training, you should get a 10GB checkpoint, it would be better if we merge it to the original 4GB checkpoint so that everything is faster</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for k in list(model_train_dict.keys()):
    if k != "state_dict":
        model_train_dict.pop(k, None)
patch_dict = model.state_dict()
new_patch_dict = {}
for i in patch_dict.keys():
    if i not in model_train_dict['state_dict'].keys():
        model_train_dict['state_dict'][i] = patch_dict[i]
torch.save(model_train_dict, "/tmp/test_merge.ckpt")
</code></pre></div></div>

<p>Now you get a 4GB checkpoint, well done</p>

<h2 id="limits-and-weak-points">Limits and weak points</h2>

<p>During the finetuning process, the stable diffusion model starts to forget other objects in the same catagory, and everything will be biased to match the new finetuning dataset</p>

<p>After finetuning on Tom And Jerry images, the model starts to draw cat &amp; kittens as tom, and cartoon bears as jerry, even without any prompt related to Tom And Jerry</p>

<p><img src="/assets/tom_biased.jpg" alt="tom_biased.jpg" /></p>

<h2 id="possible-improvements-in-the-future">Possible improvements in the future</h2>

<p><a href="https://arxiv.org/abs/2208.12242">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></p>

<p>In their research, a class-specific prior preservation loss is suggested to improve the weak points mentioned above, however, their approach requires more vram to host a original model to gather the loss</p>

<p>I don’t have the resource to do that, if you got more vram to spare, do try the class-specific prior preservation loss as optional improvement</p>

<h2 id="citations">Citations</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{gal2022textual,
      doi = {10.48550/ARXIV.2208.01618},
      url = {https://arxiv.org/abs/2208.01618},
      author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and Cohen-Or, Daniel},
      title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
      publisher = {arXiv},
      year = {2022},
      primaryClass={cs.CV}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{ruiz2022dreambooth,
  title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={arXiv preprint arxiv:2208.12242},
  year={2022}
}
</code></pre></div></div>

  </div><a class="u-url" href="/jekyll/update/2022/09/12/finetune-stable-diffusion-under-24gb-vram-in-hours.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Something I found</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Something I found</li><li><a class="u-email" href="mailto:lxj616cn@gmail.com">lxj616cn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">lxj616</span></a></li><li><a href="https://www.twitter.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">lxj616</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>There is no description for this lxj616&#39;s blog, lazy dude, nah</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
