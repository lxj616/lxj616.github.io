<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Artifact removal of text to image models using diffusion late steps | Something I found</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Artifact removal of text to image models using diffusion late steps" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Artifact Removal Of Text To Image Models Using Diffusion Late Steps" />
<meta property="og:description" content="Artifact Removal Of Text To Image Models Using Diffusion Late Steps" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html" />
<meta property="og:site_name" content="Something I found" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-14T15:01:34+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Artifact removal of text to image models using diffusion late steps" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Artifact removal of text to image models using diffusion late steps","dateModified":"2022-08-14T15:01:34+08:00","datePublished":"2022-08-14T15:01:34+08:00","url":"http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html"},"description":"Artifact Removal Of Text To Image Models Using Diffusion Late Steps","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Something I found" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Something I found</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Artifact removal of text to image models using diffusion late steps</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-08-14T15:01:34+08:00" itemprop="datePublished">Aug 14, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="artifact-removal-of-text-to-image-models-using-diffusion-late-steps">Artifact Removal Of Text To Image Models Using Diffusion Late Steps</h1>

<p>Let’s meet some example artifact images generated on craiyon(formerly dalle-mega, mega version of dalle-mini)</p>

<p><img src="/assets/craiyon_130031_a_girl_standing_in_front_of_a_car.png" alt="craiyon_130031_a_girl_standing_in_front_of_a_car.png" /></p>

<p>To be honest, these does not look good</p>

<p>And what we are doing in this blog post, is to make these images look better, effect as follows</p>

<p><img src="/assets/artifact-removal-test02.png" alt="artifact-removal-test02.png" />
<img src="/assets/artifact-removal-test01.png" alt="artifact-removal-test01.png" /></p>

<h2 id="not-a-super-resolution-upscale-problem">Not a super resolution upscale problem</h2>

<p>Many people have been using super resolution models such as ESRGAN/SwinIR to upscale their generated image to higher resolution</p>

<p>But it does not work good on these artifact images, which actually, output a more clear and high resolution nightmare image</p>

<p>A example with ESRGAN to compare with:</p>

<p><img src="/assets/craiyon_130031_a_girl_standing_in_front_of_a_car_esrgan.jpg" alt="craiyon_130031_a_girl_standing_in_front_of_a_car_esrgan.jpg" /></p>

<p>Due to the nature of image generation models, the composition itself can went all wrong in addition to wrong texture details, and depending on the model ability, the artifact area could vary from pixels to regions, in the example above, the whole face area is terrible, and obviously the whole head wasn’t at a reasonable shape in the first place</p>

<p>Super resolution does not refine the shape of the head, it does not do re-composition because current degradation method when training the SR model don’t degrade composition</p>

<h2 id="not-a-standard-image-to-image-translation-problem">Not a standard image to image translation problem</h2>

<p>The super resolution problem is one kind of image to image translation problem, from a low resolution image, to a high resolution image, not working for bad composition images</p>

<p>However, what if we try to deal with the artifact images as a img2img problem, <strong>from a bad image, to a good image</strong>, sounds good ?</p>

<p><strong>People have tried, and I do too</strong>, I tried to use vqgan-f8 as a degradation method that ends up wrong composition images, then refine it back to a vqgan-f4 image, expecting a better face for generated human figure</p>

<p>When training, results seems good, very good, images are remarkably more realistic and details are corrected, very obvious</p>

<p>When validating, results seems remain good enough, not as good as training set for certain, but most people can tell the image quality has improvement</p>

<p><strong>When testing against real artifact images, nobody can tell which is which, even the ‘refined’ image is worse</strong></p>

<p>So here I discovered the following assumptions</p>

<ol>
  <li>Artifact images contains compositional error, and it is mainly introduced by model learning not vqgan degradation</li>
  <li>There are well generated images shown remarkable quality using the same model generates artifact images, what makes it interesting is that these well generated images has significant lower vqgan-f8 degradation as well, proving there to be heavy bias during model training makes some images are a lot more concerned, and vice versa</li>
  <li>Classifier free guidance in latent-diffusion laion-400m model can generate more better quality images</li>
  <li><strong>So the artifact images are actually the outliers in the training dataset, without proper learning at both vqgan/composition stage, but close enough with the text prompt to be chosen</strong></li>
</ol>

<p>So this is not a simple image to image problem, we may regard this as ‘how to improve the original model to do better at outlier cases’</p>

<h2 id="diffusion-late-steps">Diffusion late steps</h2>

<p>Inspired by SDEdit paper, we may remove artifact of any kind by a reverse stochastic process using a diffusion model</p>

<p>The most fascinating part of this is we do not need a full diffusion model to refine the artifact image, if with luck, we only need several late steps be trained and skip the rest, this would save lots of computing power for a refinement task</p>

<p>And to further reduce the training cost, I used latent-diffusion with vqgan-f4, combined with lesser steps in the diffusion model design, it’s quite possible to finish it under low computing restrictions</p>

<p>Now the question is: how many late steps is enough ?</p>

<p>That depends on the artifact severeness, for a default 1000 steps in total, I would recommend training at 500-750 steps and skip 1-500/751-1000 at first, then finetune the 751-1000 steps training using the 500-750 steps model</p>

<p><em>I actually trained at 751-1000 then finetuned to 500-750, because I found 250 late steps is not enough</em></p>

<p>And of course, you can train 500-1000 all together, if the model has enough parameters and you got enough compute</p>

<p>Example limiting late steps when training:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()
t = torch.randint(250, 500, (x.shape[0],), device=self.device).long()
</code></pre></div></div>

<p>Example using late steps to refine when inferencing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t = repeat(torch.tensor([500]), '1 -&gt; b', b=1)
t = t.to("cuda").long()
noise = torch.randn_like(x_T)
x_T = model.q_sample(x_start=x_T, t=t, noise=noise)

#put x_T into ddim and hardcode the last steps
for i, step in enumerate(iterator):
    if last_steps:
        if i &lt; (S - last_steps):
            continue
...
</code></pre></div></div>

<p>The 500-750 steps could refine the shape of the head (middle image), then 750-1000 steps refine the details (right image)</p>

<p><img src="/assets/late_diffusion_compare.png" alt="late_diffusion_compare.png" /></p>

<h2 id="citations">Citations</h2>

<p>latent diffusion models trained using <a href="https://github.com/CompVis/latent-diffusion">https://github.com/CompVis/latent-diffusion</a> , modifications on limiting diffusion late steps</p>

<p>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations, inspired by the process of reverse stochastic process in paper, not using the code, and the training method in this blog is original not from this paper</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{https://doi.org/10.48550/arxiv.2108.01073,
  doi = {10.48550/ARXIV.2108.01073},
  
  url = {https://arxiv.org/abs/2108.01073},
  
  author = {Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
</code></pre></div></div>

  </div><a class="u-url" href="/jekyll/update/2022/08/14/artifact-removal-of-text-to-image-models-using-diffusion-late-steps.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Something I found</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Something I found</li><li><a class="u-email" href="mailto:lxj616cn@gmail.com">lxj616cn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">lxj616</span></a></li><li><a href="https://www.twitter.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">lxj616</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>There is no description for this lxj616&#39;s blog, lazy dude, nah</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
