<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Make a longer stable diffusion video on home computers | Something I found</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Make a longer stable diffusion video on home computers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Make A Longer Stable Diffusion Video On Home Computers" />
<meta property="og:description" content="Make A Longer Stable Diffusion Video On Home Computers" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html" />
<meta property="og:site_name" content="Something I found" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-03-05T09:37:50+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Make a longer stable diffusion video on home computers" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Make a longer stable diffusion video on home computers","dateModified":"2023-03-05T09:37:50+08:00","datePublished":"2023-03-05T09:37:50+08:00","url":"http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html"},"description":"Make A Longer Stable Diffusion Video On Home Computers","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Something I found" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Something I found</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Make a longer stable diffusion video on home computers</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-03-05T09:37:50+08:00" itemprop="datePublished">Mar 5, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="make-a-longer-stable-diffusion-video-on-home-computers">Make A Longer Stable Diffusion Video On Home Computers</h1>

<p><img src="/assets/trump_small.gif" alt="trump_small.gif" />
<img src="/assets/spiderman3_small.gif" alt="spiderman3_small.gif" />
<img src="/assets/ironman_small.gif" alt="ironman_small.gif" />
<img src="/assets/spiderman2_small.gif" alt="spiderman2_small.gif" /></p>

<p>The gif is manually resized to 256x256 and heavy lossy compressed using <code class="language-plaintext highlighter-rouge">gifsicle -O3 --lossy=200 --resize 256x256</code> for better blog network loading speed, originally trained at 512x512 and 64 frames</p>

<p>However these gifs are more than 1MB each, so if you have trouble loading the gif, you may need to go and download from github blog repo yourself, I can’t compress the gifs any further</p>

<p><strong>The dataset and model heavy resemble real life human with personal identity such as faces and bodys, thus can not go opensource for legal concerns</strong></p>

<h2 id="longer-problem-for-longer-video">Longer Problem For Longer Video</h2>

<ol>
  <li>
    <p>It simply just won’t work if you set frames_length to higher value and press enter harder with your finger</p>
  </li>
  <li>
    <p>My previous timelapse toy model used timelapse video dataset, although they have adequate clip length for longer video experiment, it doesn’t make sense training longer sequence when shorter is good enough</p>
  </li>
  <li>
    <p>As in tradition, one single RTX 3090Ti (24 GB vram) is what I got, and all the fancy longer video generation stuff, I mean it to get it done with the exact same home computer computation limitations</p>
  </li>
</ol>

<h2 id="the-missing-make-a-video-technique">The Missing Make A Video Technique</h2>

<p>Well, I already give out make-a-stable-diffusion-video github repo to demonstrate how to make it work, especially on home computers</p>

<p>And I stated that in my last blog post: ‘Oh, I can not afford that with 24gb vram, let’s just pretend there isn’t a whole paragraph in make-a-video paper explaining video frame interpolation’</p>

<p>Now that I’m gonna try more frames and wish to get coherent results for a long range of frames, and probably finish training myself instead of leaving a letter to my grandson</p>

<p><strong>Video frame interpolation is what I need, the missing piece</strong></p>

<p><a href="https://paperswithcode.com/task/video-frame-interpolation">https://paperswithcode.com/task/video-frame-interpolation</a></p>

<p>So I made a hack to my code, using the inpainting model special feature to implement fast and incorrect interpolation</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hint_latents = latents[:,:,0::4,:,:]
hint_latents_expand = hint_latents.repeat_interleave(4,2)
hint_latents_expand = hint_latents_expand[:,:,:args.frames_length,:,:]
latent_model_input = torch.cat([noisy_latents, masks_input, hint_latents_expand], dim=1).to(accelerator.device)
</code></pre></div></div>

<p>Well, for every 4 frames, set the original frame to inpainting condition input to generate exact frame image since I masked nothing</p>

<p>Good thing is this hack is almost one line without custom attention module modification, bad thing is this is mathematically wrong because I really should set the static frame without model backbone inference on it</p>

<p>And as for duplicating the hint input to every subordinate frames, I didn’t do research on its effects, I can’t answer it because I have no clue myself</p>

<p>So, here is the plan:</p>

<ol>
  <li>generate 5 frames</li>
  <li>interpolate to 17 frames, (5-1)x4+1=17</li>
  <li>interpolate to 65 frames, (17-1)x4+1=65</li>
</ol>

<h2 id="attention-is-all-i-can-not-afford">Attention Is All I Can Not Afford</h2>

<p>Surely I wouldn’t meet many trouble dealing with 5 frames, whatever attention I use</p>

<p>But 65 frames leads to a huge problem, especially when we do interpolation</p>

<p>Normal attention has O(n^2) complexity</p>

<p><strong>For 5 frames, n is 5, so that would be 5^2=25 units of complexity</strong>
<strong>For 65 frames, n is 65, so that would be 65^2=4225 units of complexity</strong></p>

<p>So it is obvious I need to train 65 frames model 169x times than the 5 frames model, this could be a problem</p>

<p>I actually tried that for comparison, the 65 frames model is a total mess visually and by loss curve, even already trained for 20 epochs, I should have saved the screenshot, but I get too frustrated and forgot</p>

<p>Here comes a better(?) attention mechanism for long sequences</p>

<p><a href="https://paperswithcode.com/method/sliding-window-attention">https://paperswithcode.com/method/sliding-window-attention</a></p>

<p>With sliding window attention such as local attention, you only need O(n x w) complexity, for 65 frames and windowsize 5, its 65x5=325 units of complexity, compared to 4225 it almost seem like a silver bullet</p>

<p>But, does it ?</p>

<p>By using <a href="https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/local.py">https://github.com/facebookresearch/xformers/blob/main/xformers/components/attention/local.py</a></p>

<p><strong>I found the shortcomings of local attention, which is too narrow minded on adjacent frames, the most notable effect is the identity of the main subject changes rapidly across frames</strong></p>

<p>Good thing is local attention’s shortcomings are trival to our interpolation network, we always have a reference frame every 4th frames, so no need to worry about identity change</p>

<p>Note: I have no idea what windowsize is the best, I just used 5 which is the default value</p>

<p>It turned out by using local attention, I am being able to train the 65 frames interpolation network for 20 epochs and seems to be good enough (my fancy way of saying I ran out of patience and stopped)</p>

<h2 id="a-not-too-short-and-not-too-small-dataset-for-testing">A Not Too Short And Not Too Small Dataset For Testing</h2>

<p>Many would ask: what’s wrong with the timelapse video dataset, it’s long enough and you already have that, why bother making another dataset ?</p>

<ol>
  <li>timelapse video dataset is too small (286 videos), it does not generalize well to make creativity art, a cat standing idle with clouds moving is almost the only thing it does</li>
  <li>stable diffusion is good at generating landscape images, and human eyes are not sensitive about nature scenes, nature landscape change very little across time, a timelapse dataset can cover hidden technical problems, but this time I shall face the real challenge</li>
</ol>

<p>So, this time I made a ‘fashion model walking on stage’ video dataset, it has the following features:</p>

<ol>
  <li>2848 videos, all above 100 frames, almost 10x the size of timelapse video dataset</li>
  <li>contains human, a forbidden area for stable diffusion both for poor generation quality and for legal obligations</li>
  <li>human changes scale across time, far to near, not the same size</li>
</ol>

<p><strong>Speak of legal obligations, I am not being able to opensource the dataset or the model, because it is trained on human, it has to more or less contain personal identity information such as faces and bodys</strong></p>

<p><strong>All I could publish is where I got the raw videos:</strong></p>

<p><a href="https://www.youtube.com/@yaersfashiontv">https://www.youtube.com/@yaersfashiontv</a></p>

<p>And I used blender to preprocess the videos, manually</p>

<h2 id="unverified-experimental-hacks">Unverified experimental hacks</h2>

<p>Except for what I did and tested above, I actually experimented custom attention patterns like always attend to first frame no matter what window size</p>

<p>But I can not tell the difference, so without proof I can just say I am not able to confirm whether they work or not</p>

<h2 id="recommended-opensource-implementations">Recommended Opensource Implementations</h2>

<p>I have noticed that there is a cleaner and easier to use implementation other than my make-a-stable-diffusion-video repo</p>

<p><strong>If you got enough vram and wish not to use my hacks (which mainly focus on running under 24GB vram), you can check this work in progress implementation by chavinlo</strong></p>

<p><a href="https://github.com/chavinlo/TempoFunk">https://github.com/chavinlo/TempoFunk</a></p>

<h2 id="citations">Citations</h2>

<p>Thanks to the opensource repos made by <a href="https://github.com/lucidrains">https://github.com/lucidrains</a> , including but not limited to:</p>

<p>https://github.com/lucidrains/make-a-video-pytorch</p>

<p>https://github.com/lucidrains/video-diffusion-pytorch</p>

<p>And my code is based on <a href="https://github.com/huggingface/diffusers">https://github.com/huggingface/diffusers</a>, especially most of the speed up tricks are bundled within the original repository</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Singer2022,
    author  = {Uriel Singer},
    url     = {https://makeavideo.studio/Make-A-Video.pdf}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{ho2022video,
  title   = {Video Diffusion Models}, 
  author  = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  year    = {2022},
  eprint  = {2204.03458},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{von-platen-etal-2022-diffusers,
  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
  title = {Diffusers: State-of-the-art diffusion models},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/diffusers}}
}
</code></pre></div></div>

  </div><a class="u-url" href="/jekyll/update/2023/03/05/make-a-longer-stable-diffusion-video.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Something I found</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Something I found</li><li><a class="u-email" href="mailto:lxj616cn@gmail.com">lxj616cn@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">lxj616</span></a></li><li><a href="https://www.twitter.com/lxj616"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">lxj616</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>There is no description for this lxj616&#39;s blog, lazy dude, nah</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
